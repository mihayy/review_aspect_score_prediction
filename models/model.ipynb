{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "3yfZQ1uCd9P-",
        "vhWNtM5T79-Q",
        "pCHVANHx8FLu",
        "vc0MJancv5xM"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mihayy/review_aspect_score_prediction/blob/master/models/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le5n9ehPMdbf",
        "colab_type": "text"
      },
      "source": [
        "# Neural Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDFdOmvwdmbw",
        "colab_type": "text"
      },
      "source": [
        "### import libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5d741f2f-8527-4d53-b954-fde2199413f5",
        "id": "rflOk9zEqEB2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "!pip install opt-einsum==2.3.2 tb-nightly==1.15.0a20190730 tf-estimator-nightly==1.14.0.dev2019073001 tf-nightly-gpu==1.15.0.dev20190730"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opt-einsum==2.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/d6/44792ec668bcda7d91913c75237314e688f70415ab2acd7172c845f0b24f/opt_einsum-2.3.2.tar.gz (59kB)\n",
            "\r\u001b[K     |█████▌                          | 10kB 20.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 20kB 26.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 30kB 33.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 40kB 31.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 51kB 35.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 25.8MB/s \n",
            "\u001b[?25hCollecting tb-nightly==1.15.0a20190730\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/9b/656b60df782ab795cb998139399f5e52595bab61826aa94e622a0756e3db/tb_nightly-1.15.0a20190730-py3-none-any.whl (4.1MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1MB 43.9MB/s \n",
            "\u001b[?25hCollecting tf-estimator-nightly==1.14.0.dev2019073001\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/38/2b37afaa7ff170cc96a183bd897d02f9e91a0ac62fd477bad7452fdc9051/tf_estimator_nightly-1.14.0.dev2019073001-py2.py3-none-any.whl (501kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 47.6MB/s \n",
            "\u001b[?25hCollecting tf-nightly-gpu==1.15.0.dev20190730\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/ac/0fc5f3fc4c60be4c5f8368df34359c47192de856a9a735628536461a8eb9/tf_nightly_gpu-1.15.0.dev20190730-cp36-cp36m-manylinux1_x86_64.whl (406.6MB)\n",
            "\u001b[K     |████████████████████████████████| 406.7MB 62kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from opt-einsum==2.3.2) (1.16.4)\n",
            "Requirement already satisfied: grpcio>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly==1.15.0a20190730) (1.15.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly==1.15.0a20190730) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly==1.15.0a20190730) (3.7.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly==1.15.0a20190730) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly==1.15.0a20190730) (0.15.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly==1.15.0a20190730) (41.2.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tb-nightly==1.15.0a20190730) (0.33.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tb-nightly==1.15.0a20190730) (0.7.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu==1.15.0.dev20190730) (1.11.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu==1.15.0.dev20190730) (0.2.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu==1.15.0.dev20190730) (0.1.7)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu==1.15.0.dev20190730) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu==1.15.0.dev20190730) (1.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu==1.15.0.dev20190730) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu==1.15.0.dev20190730) (0.8.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tf-nightly-gpu==1.15.0.dev20190730) (2.8.0)\n",
            "Building wheels for collected packages: opt-einsum\n",
            "  Building wheel for opt-einsum (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for opt-einsum: filename=opt_einsum-2.3.2-cp36-none-any.whl size=49883 sha256=a85415f72ec0397f28c6bc7e60fb64e59d1e7a63410c8dab87b7d65f3afd9d7f\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/3e/a3/b351fae0cbf15373c2136a54a70f43fea5fe91d8168a5faaa4\n",
            "Successfully built opt-einsum\n",
            "Installing collected packages: opt-einsum, tb-nightly, tf-estimator-nightly, tf-nightly-gpu\n",
            "  Found existing installation: opt-einsum 3.0.1\n",
            "    Uninstalling opt-einsum-3.0.1:\n",
            "      Successfully uninstalled opt-einsum-3.0.1\n",
            "Successfully installed opt-einsum-2.3.2 tb-nightly-1.15.0a20190730 tf-estimator-nightly-1.14.0.dev2019073001 tf-nightly-gpu-1.15.0.dev20190730\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaSd4E5gU8qe",
        "colab_type": "code",
        "outputId": "601b8e65-6d9d-4cc3-f368-2f0d476bb78e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import nltk\n",
        "import torchtext\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from tensorflow.contrib.layers import fully_connected\n",
        "import tensorflow as tf\n",
        "from google.colab import files\n",
        "import os\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "from tensorflow.python.ops.metrics_impl import _streaming_confusion_matrix\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "%load_ext tensorboard\n",
        "# %load_ext tensorboard.notebook"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrD7Z-aRdg3i",
        "colab_type": "code",
        "outputId": "9755696f-b3b4-4a95-c58b-644fe6e5cb8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip list"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Package                  Version              \n",
            "------------------------ ---------------------\n",
            "absl-py                  0.7.1                \n",
            "alabaster                0.7.12               \n",
            "albumentations           0.1.12               \n",
            "altair                   3.2.0                \n",
            "astor                    0.8.0                \n",
            "astropy                  3.0.5                \n",
            "atari-py                 0.1.15               \n",
            "atomicwrites             1.3.0                \n",
            "attrs                    19.1.0               \n",
            "audioread                2.1.8                \n",
            "autograd                 1.3                  \n",
            "Babel                    2.7.0                \n",
            "backcall                 0.1.0                \n",
            "backports.tempfile       1.0                  \n",
            "backports.weakref        1.0.post1            \n",
            "beautifulsoup4           4.6.3                \n",
            "bleach                   3.1.0                \n",
            "blis                     0.2.4                \n",
            "bokeh                    1.0.4                \n",
            "boto                     2.49.0               \n",
            "boto3                    1.9.205              \n",
            "botocore                 1.12.205             \n",
            "Bottleneck               1.2.1                \n",
            "branca                   0.3.1                \n",
            "bs4                      0.0.1                \n",
            "bz2file                  0.98                 \n",
            "cachetools               3.1.1                \n",
            "certifi                  2019.6.16            \n",
            "cffi                     1.12.3               \n",
            "chainer                  5.4.0                \n",
            "chardet                  3.0.4                \n",
            "Click                    7.0                  \n",
            "cloudpickle              0.6.1                \n",
            "cmake                    3.12.0               \n",
            "colorlover               0.3.0                \n",
            "community                1.0.0b1              \n",
            "contextlib2              0.5.5                \n",
            "convertdate              2.1.3                \n",
            "coverage                 3.7.1                \n",
            "coveralls                0.5                  \n",
            "crcmod                   1.7                  \n",
            "cufflinks                0.14.6               \n",
            "cupy-cuda100             5.4.0                \n",
            "cvxopt                   1.2.3                \n",
            "cvxpy                    1.0.24               \n",
            "cycler                   0.10.0               \n",
            "cymem                    2.0.2                \n",
            "Cython                   0.29.13              \n",
            "daft                     0.0.4                \n",
            "dask                     1.1.5                \n",
            "dataclasses              0.6                  \n",
            "datascience              0.10.6               \n",
            "decorator                4.4.0                \n",
            "defusedxml               0.6.0                \n",
            "descartes                1.1.0                \n",
            "dill                     0.3.0                \n",
            "distributed              1.25.3               \n",
            "Django                   2.2.4                \n",
            "dlib                     19.16.0              \n",
            "dm-sonnet                1.34                 \n",
            "docopt                   0.6.2                \n",
            "docutils                 0.14                 \n",
            "dopamine-rl              1.0.5                \n",
            "easydict                 1.9                  \n",
            "ecos                     2.0.7.post1          \n",
            "editdistance             0.5.3                \n",
            "en-core-web-sm           2.1.0                \n",
            "entrypoints              0.3                  \n",
            "ephem                    3.7.6.0              \n",
            "et-xmlfile               1.0.1                \n",
            "fa2                      0.3.5                \n",
            "fancyimpute              0.4.3                \n",
            "fastai                   1.0.57               \n",
            "fastdtw                  0.3.2                \n",
            "fastprogress             0.1.21               \n",
            "fastrlock                0.4                  \n",
            "fbprophet                0.5                  \n",
            "feather-format           0.4.0                \n",
            "featuretools             0.4.1                \n",
            "filelock                 3.0.12               \n",
            "fix-yahoo-finance        0.0.22               \n",
            "Flask                    1.1.1                \n",
            "folium                   0.8.3                \n",
            "fsspec                   0.4.1                \n",
            "future                   0.16.0               \n",
            "gast                     0.2.2                \n",
            "GDAL                     2.2.2                \n",
            "gdown                    3.6.4                \n",
            "gensim                   3.6.0                \n",
            "geographiclib            1.49                 \n",
            "geopy                    1.17.0               \n",
            "gevent                   1.4.0                \n",
            "gin-config               0.2.0                \n",
            "glob2                    0.7                  \n",
            "google                   2.0.2                \n",
            "google-api-core          1.14.2               \n",
            "google-api-python-client 1.7.10               \n",
            "google-auth              1.4.2                \n",
            "google-auth-httplib2     0.0.3                \n",
            "google-auth-oauthlib     0.4.0                \n",
            "google-cloud-bigquery    1.14.0               \n",
            "google-cloud-core        1.0.3                \n",
            "google-cloud-datastore   1.8.0                \n",
            "google-cloud-language    1.2.0                \n",
            "google-cloud-storage     1.16.1               \n",
            "google-cloud-translate   1.5.0                \n",
            "google-colab             1.0.0                \n",
            "google-pasta             0.1.7                \n",
            "google-resumable-media   0.3.2                \n",
            "googleapis-common-protos 1.6.0                \n",
            "googledrivedownloader    0.4                  \n",
            "graph-nets               1.0.4                \n",
            "graphviz                 0.10.1               \n",
            "greenlet                 0.4.15               \n",
            "grpcio                   1.15.0               \n",
            "gspread                  3.0.1                \n",
            "gspread-dataframe        3.0.3                \n",
            "gunicorn                 19.9.0               \n",
            "gym                      0.10.11              \n",
            "h5py                     2.8.0                \n",
            "HeapDict                 1.0.0                \n",
            "holidays                 0.9.11               \n",
            "html5lib                 1.0.1                \n",
            "httpimport               0.5.16               \n",
            "httplib2                 0.11.3               \n",
            "humanize                 0.5.1                \n",
            "hyperopt                 0.1.2                \n",
            "ideep4py                 2.0.0.post3          \n",
            "idna                     2.8                  \n",
            "image                    1.5.27               \n",
            "imageio                  2.4.1                \n",
            "imagesize                1.1.0                \n",
            "imbalanced-learn         0.4.3                \n",
            "imblearn                 0.0                  \n",
            "imgaug                   0.2.9                \n",
            "importlib-metadata       0.19                 \n",
            "imutils                  0.5.2                \n",
            "inflect                  2.1.0                \n",
            "intel-openmp             2019.0               \n",
            "intervaltree             2.1.0                \n",
            "ipykernel                4.6.1                \n",
            "ipython                  5.5.0                \n",
            "ipython-genutils         0.2.0                \n",
            "ipython-sql              0.3.9                \n",
            "ipywidgets               7.5.1                \n",
            "itsdangerous             1.1.0                \n",
            "jdcal                    1.4.1                \n",
            "jedi                     0.15.1               \n",
            "jieba                    0.39                 \n",
            "Jinja2                   2.10.1               \n",
            "jmespath                 0.9.4                \n",
            "joblib                   0.13.2               \n",
            "jpeg4py                  0.1.4                \n",
            "jsonschema               2.6.0                \n",
            "jupyter                  1.0.0                \n",
            "jupyter-client           5.3.1                \n",
            "jupyter-console          5.2.0                \n",
            "jupyter-core             4.5.0                \n",
            "kaggle                   1.5.5                \n",
            "kapre                    0.1.3.1              \n",
            "Keras                    2.2.4                \n",
            "Keras-Applications       1.0.8                \n",
            "Keras-Preprocessing      1.1.0                \n",
            "keras-vis                0.4.1                \n",
            "kiwisolver               1.1.0                \n",
            "knnimpute                0.1.0                \n",
            "librosa                  0.6.3                \n",
            "lightgbm                 2.2.3                \n",
            "llvmlite                 0.29.0               \n",
            "lmdb                     0.96                 \n",
            "lucid                    0.3.8                \n",
            "lunardate                0.2.0                \n",
            "lxml                     4.2.6                \n",
            "magenta                  0.3.19               \n",
            "Markdown                 3.1.1                \n",
            "MarkupSafe               1.1.1                \n",
            "matplotlib               3.0.3                \n",
            "matplotlib-venn          0.11.5               \n",
            "mesh-tensorflow          0.0.5                \n",
            "mido                     1.2.6                \n",
            "mir-eval                 0.5                  \n",
            "missingno                0.4.2                \n",
            "mistune                  0.8.4                \n",
            "mizani                   0.5.4                \n",
            "mkl                      2019.0               \n",
            "mlxtend                  0.14.0               \n",
            "more-itertools           7.2.0                \n",
            "moviepy                  0.2.3.5              \n",
            "mpi4py                   3.0.2                \n",
            "mpmath                   1.1.0                \n",
            "msgpack                  0.5.6                \n",
            "multiprocess             0.70.8               \n",
            "multitasking             0.0.9                \n",
            "murmurhash               1.0.2                \n",
            "music21                  5.5.0                \n",
            "natsort                  5.5.0                \n",
            "nbconvert                5.6.0                \n",
            "nbformat                 4.4.0                \n",
            "networkx                 2.3                  \n",
            "nibabel                  2.3.3                \n",
            "nltk                     3.2.5                \n",
            "nose                     1.3.7                \n",
            "notebook                 5.2.2                \n",
            "np-utils                 0.5.10.0             \n",
            "numba                    0.40.1               \n",
            "numexpr                  2.6.9                \n",
            "numpy                    1.16.4               \n",
            "nvidia-ml-py3            7.352.0              \n",
            "oauth2client             4.1.3                \n",
            "oauthlib                 3.1.0                \n",
            "okgrade                  0.4.3                \n",
            "olefile                  0.46                 \n",
            "opencv-contrib-python    3.4.3.18             \n",
            "opencv-python            3.4.5.20             \n",
            "openpyxl                 2.5.9                \n",
            "opt-einsum               2.3.2                \n",
            "osqp                     0.5.0                \n",
            "packaging                19.1                 \n",
            "palettable               3.2.0                \n",
            "pandas                   0.24.2               \n",
            "pandas-datareader        0.7.4                \n",
            "pandas-gbq               0.4.1                \n",
            "pandas-profiling         1.4.1                \n",
            "pandocfilters            1.4.2                \n",
            "parso                    0.5.1                \n",
            "pathlib                  1.0.1                \n",
            "patsy                    0.5.1                \n",
            "pexpect                  4.7.0                \n",
            "pickleshare              0.7.5                \n",
            "Pillow                   4.3.0                \n",
            "pip                      19.2.1               \n",
            "pip-tools                3.9.0                \n",
            "plac                     0.9.6                \n",
            "plotly                   3.6.1                \n",
            "plotnine                 0.5.1                \n",
            "pluggy                   0.7.1                \n",
            "portpicker               1.2.0                \n",
            "prefetch-generator       1.0.1                \n",
            "preshed                  2.0.1                \n",
            "pretty-midi              0.2.8                \n",
            "prettytable              0.7.2                \n",
            "progressbar2             3.38.0               \n",
            "prometheus-client        0.7.1                \n",
            "promise                  2.2.1                \n",
            "prompt-toolkit           1.0.16               \n",
            "protobuf                 3.7.1                \n",
            "psutil                   5.4.8                \n",
            "psycopg2                 2.7.6.1              \n",
            "ptyprocess               0.6.0                \n",
            "py                       1.8.0                \n",
            "pyarrow                  0.14.1               \n",
            "pyasn1                   0.4.6                \n",
            "pyasn1-modules           0.2.6                \n",
            "pycocotools              2.0.0                \n",
            "pycparser                2.19                 \n",
            "pydot                    1.3.0                \n",
            "pydot-ng                 2.0.0                \n",
            "pydotplus                2.0.2                \n",
            "pyemd                    0.5.1                \n",
            "pyglet                   1.4.1                \n",
            "Pygments                 2.1.3                \n",
            "pygobject                3.26.1               \n",
            "pymc3                    3.7                  \n",
            "pymongo                  3.8.0                \n",
            "pymystem3                0.2.0                \n",
            "PyOpenGL                 3.1.0                \n",
            "pyparsing                2.4.2                \n",
            "pyrsistent               0.15.4               \n",
            "pysndfile                1.3.7                \n",
            "PySocks                  1.7.0                \n",
            "pystan                   2.19.0.0             \n",
            "pytest                   3.6.4                \n",
            "python-apt               1.6.4                \n",
            "python-chess             0.23.11              \n",
            "python-dateutil          2.5.3                \n",
            "python-louvain           0.13                 \n",
            "python-rtmidi            1.3.0                \n",
            "python-slugify           3.0.3                \n",
            "python-utils             2.3.0                \n",
            "pytz                     2018.9               \n",
            "PyWavelets               1.0.3                \n",
            "PyYAML                   3.13                 \n",
            "pyzmq                    17.0.0               \n",
            "qtconsole                4.5.2                \n",
            "requests                 2.21.0               \n",
            "requests-oauthlib        1.2.0                \n",
            "resampy                  0.2.1                \n",
            "retrying                 1.3.3                \n",
            "rpy2                     2.9.5                \n",
            "rsa                      4.0                  \n",
            "s3fs                     0.3.3                \n",
            "s3transfer               0.2.1                \n",
            "scikit-image             0.15.0               \n",
            "scikit-learn             0.21.3               \n",
            "scipy                    1.3.1                \n",
            "screen-resolution-extra  0.0.0                \n",
            "scs                      2.1.1.post2          \n",
            "seaborn                  0.9.0                \n",
            "semantic-version         2.6.0                \n",
            "Send2Trash               1.5.0                \n",
            "setuptools               41.0.1               \n",
            "setuptools-git           1.2                  \n",
            "Shapely                  1.6.4.post2          \n",
            "simplegeneric            0.8.1                \n",
            "six                      1.12.0               \n",
            "sklearn                  0.0                  \n",
            "sklearn-pandas           1.8.0                \n",
            "smart-open               1.8.4                \n",
            "snowballstemmer          1.9.0                \n",
            "sortedcontainers         2.1.0                \n",
            "spacy                    2.1.8                \n",
            "Sphinx                   1.8.5                \n",
            "sphinxcontrib-websupport 1.1.2                \n",
            "SQLAlchemy               1.3.6                \n",
            "sqlparse                 0.3.0                \n",
            "srsly                    0.0.7                \n",
            "stable-baselines         2.2.1                \n",
            "statsmodels              0.10.1               \n",
            "sympy                    1.1.1                \n",
            "tables                   3.4.4                \n",
            "tabulate                 0.8.3                \n",
            "tb-nightly               1.15.0a20190730      \n",
            "tblib                    1.4.0                \n",
            "tensor2tensor            1.11.0               \n",
            "tensorboard              1.14.0               \n",
            "tensorboardcolab         0.0.22               \n",
            "tensorflow               1.14.0               \n",
            "tensorflow-estimator     1.14.0               \n",
            "tensorflow-hub           0.5.0                \n",
            "tensorflow-metadata      0.14.0               \n",
            "tensorflow-probability   0.7.0                \n",
            "termcolor                1.1.0                \n",
            "terminado                0.8.2                \n",
            "testpath                 0.4.2                \n",
            "text-unidecode           1.2                  \n",
            "textblob                 0.15.3               \n",
            "textgenrnn               1.4.1                \n",
            "tf-estimator-nightly     1.14.0.dev2019073001 \n",
            "tf-nightly-gpu           1.15.0.dev20190730   \n",
            "tfds-nightly             1.1.0.dev201908090105\n",
            "tflearn                  0.3.2                \n",
            "Theano                   1.0.4                \n",
            "thinc                    7.0.8                \n",
            "toolz                    0.10.0               \n",
            "torch                    1.1.0                \n",
            "torchsummary             1.5.1                \n",
            "torchtext                0.3.1                \n",
            "torchvision              0.3.0                \n",
            "tornado                  4.5.3                \n",
            "tqdm                     4.28.1               \n",
            "traitlets                4.3.2                \n",
            "tweepy                   3.6.0                \n",
            "typing                   3.7.4                \n",
            "tzlocal                  1.5.1                \n",
            "umap-learn               0.3.9                \n",
            "uritemplate              3.0.0                \n",
            "urllib3                  1.24.3               \n",
            "vega-datasets            0.7.0                \n",
            "wasabi                   0.2.2                \n",
            "wcwidth                  0.1.7                \n",
            "webencodings             0.5.1                \n",
            "Werkzeug                 0.15.5               \n",
            "wheel                    0.33.4               \n",
            "widgetsnbextension       3.5.1                \n",
            "wordcloud                1.5.0                \n",
            "wrapt                    1.11.2               \n",
            "xarray                   0.11.3               \n",
            "xgboost                  0.90                 \n",
            "xkit                     0.0.0                \n",
            "xlrd                     1.1.0                \n",
            "xlwt                     1.3.0                \n",
            "yellowbrick              0.9.1                \n",
            "zict                     1.0.0                \n",
            "zipp                     0.5.2                \n",
            "zmq                      0.0.0                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6QsRY3TSI-T",
        "colab_type": "code",
        "outputId": "370ccb0a-18f2-47a2-b1e3-5d06899e1493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != \"/device:GPU:0\":\n",
        "  print(\"Device not found!\")\n",
        "else:\n",
        "  print(device_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2b6WuhSx2tU",
        "colab_type": "text"
      },
      "source": [
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAL0bJnFx7OL",
        "colab_type": "code",
        "outputId": "e0241a84-083d-4641-d694-b662f9be534e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nl6jE0SDU8qj",
        "colab_type": "text"
      },
      "source": [
        "### General environment variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH7RLrf8U8qk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Config(object):\n",
        "  def __init__(self):\n",
        "    self.gdrive_working_dir = Path(\"gdrive/My Drive/thesis\")\n",
        "\n",
        "    self.embeddings_path = self.gdrive_working_dir/ \"embeddings\"\n",
        "\n",
        "#     self.dataset_intermediate_path = Path(\"manual_labeled_strength_weak_sections/summary_strength_weak_sections\")\n",
        "    self.dataset_intermediate_path = Path(\"\")\n",
        "\n",
        "#     self.dataset_intermediate_path = Path(\"manual_labeled_strength_weak_sections\")\n",
        "#     self.dataset_intermediate_path = Path(\"acl_abstracts\")\n",
        "    self.dataset_path = self.gdrive_working_dir/ \"data\" / self.dataset_intermediate_path\n",
        "\n",
        "    self.train_dataset_filename = \"train_dataset.csv\"\n",
        "    self.dev_dataset_filename = \"dev_dataset.csv\"\n",
        "    self.test_dataset_filename = \"acl_dev_test.csv\"\n",
        "    \n",
        "    self.train_ds = \"train_ds_\"\n",
        "    self.test_ds = \"test_ds_\"\n",
        "\n",
        "    self.aspects_no_com_approp = ['RECOMMENDATION', 'REVIEWER_CONFIDENCE', 'SOUNDNESS_CORRECTNESS', 'IMPACT', 'SUBSTANCE', 'CLARITY', 'ORIGINALITY']\n",
        "    \n",
        "    self.stats_path = self.gdrive_working_dir / \"tensorflow_stats_random\"\n",
        "#     self.stats_path = Path(\"\")\n",
        "    \n",
        "  def set_dataset_intermediate_path(self, intermediate_path):\n",
        "    self.dataset_intermediate_path = intermediate_path\n",
        "    self.dataset_path = self.gdrive_working_dir/ \"data\" / self.dataset_intermediate_path\n",
        "    \n",
        "  def set_ds_fname(self, iteration):\n",
        "    self.train_dataset_filename = self.train_ds + str(iteration) + \".csv\"\n",
        "    self.dev_dataset_filename = self.test_ds + str(iteration) + \".csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGb6APcqEjhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_var = Config()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yfZQ1uCd9P-",
        "colab_type": "text"
      },
      "source": [
        "### Dataset manipulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhWNtM5T79-Q",
        "colab_type": "text"
      },
      "source": [
        "#### Section manipulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIjLMlZ55NFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from string import punctuation\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7kDS3DvS0JE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Review_Section(object):\n",
        "  def __init__(self):\n",
        "    self.any_strength_weak_sections = False\n",
        "    self.is_strength_present = False\n",
        "    self.is_weakness_present = False\n",
        "    self.is_discussion_present = False\n",
        "\n",
        "    self.strength_idx = 0\n",
        "    self.weakness_idx = 0\n",
        "    self.discussion_idx = None\n",
        "\n",
        "    self.relevant_sections = []\n",
        "    self.init_sections = []\n",
        "    self.first_section = \"\"\n",
        "    self.last_section = \"\"\n",
        "    self.filtered_in_between_sections = []\n",
        "    \n",
        "    self.irrelevant_section_keywords = [\"minor\", \"detailed\", \"reference\", \"other\", \"question\", \"small\", \"author\", \"further\", \"additional\"]\n",
        "    \n",
        "    self.section_len = 170\n",
        "    \n",
        "  def try_detect_strength_weak_sections(self, sections):\n",
        "    for idx, section in enumerate(sections):\n",
        "      strip_lower_section = section.strip().lower()\n",
        "      if \"weak\" in strip_lower_section:\n",
        "        self.is_weakness_present = True\n",
        "        self.any_strength_weak_sections = True\n",
        "        self.weakness_idx = idx\n",
        "\n",
        "      if \"stren\" in strip_lower_section:\n",
        "        self.is_strength_present = True\n",
        "        self.any_strength_weak_sections = True\n",
        "        self.strength_idx = idx\n",
        "\n",
        "      if \"general discussion\" in strip_lower_section:\n",
        "        self.is_discussion_present = True     \n",
        "        self.discussion_idx = idx\n",
        "  \n",
        "  def compute_strength_section(self, sections):\n",
        "    if self.is_strength_present:\n",
        "      self.first_section = sections[self.strength_idx]\n",
        "\n",
        "    in_between_sections = sections[self.strength_idx:self.discussion_idx] \n",
        "\n",
        "    if self.is_discussion_present:\n",
        "      self.last_section = sections[self.discussion_idx]\n",
        "\n",
        "    self.relevant_sections = self.init_sections\n",
        "\n",
        "    for section in in_between_sections:\n",
        "      section_strip_lower = section.strip().lower()\n",
        "      is_section_irrelevant = [keyword in section_strip_lower for keyword in self.irrelevant_section_keywords + [\"weakness\"]]\n",
        "      if any(is_section_irrelevant):\n",
        "        self.last_section = section\n",
        "        break  \n",
        "      self.relevant_sections.append(section)\n",
        "      \n",
        "  def compute_summary_section(self, sections):\n",
        "    in_between_sections = sections[:self.discussion_idx] \n",
        "    \n",
        "    if self.is_discussion_present:\n",
        "      self.last_section = sections[self.discussion_idx]\n",
        "    \n",
        "    if self.is_strength_present:\n",
        "      self.last_section = sections[self.strength_idx]\n",
        "      in_between_sections = sections[:self.strength_idx] \n",
        "\n",
        "    self.relevant_sections = self.init_sections\n",
        "\n",
        "    for section in in_between_sections:\n",
        "      section_strip_lower = section.strip().lower()\n",
        "      is_section_irrelevant = [keyword in section_strip_lower for keyword in self.irrelevant_section_keywords + [\"weakness\"]]\n",
        "      if any(is_section_irrelevant):\n",
        "        self.last_section = section\n",
        "        break  \n",
        "      self.relevant_sections.append(section)\n",
        "      \n",
        "  def compute_weakness_section(self, sections):\n",
        "\n",
        "    in_between_sections = sections[:self.discussion_idx]\n",
        "\n",
        "    if self.is_discussion_present:\n",
        "      self.last_section = sections[self.discussion_idx]\n",
        "\n",
        "    if self.is_weakness_present:\n",
        "      self.first_section = sections[self.weakness_idx]\n",
        "      \n",
        "      in_between_sections = sections[self.weakness_idx+1:self.discussion_idx] \n",
        "      \n",
        "    self.relevant_sections = self.init_sections\n",
        "\n",
        "    for section in in_between_sections:\n",
        "      section_strip_lower = section.strip().lower()\n",
        "      is_section_irrelevant = [keyword in section_strip_lower for keyword in self.irrelevant_section_keywords]\n",
        "      if any(is_section_irrelevant):\n",
        "        self.last_section = section\n",
        "        break  \n",
        "      self.relevant_sections.append(section)\n",
        "  \n",
        "  def compute_strength_weak_section(self, sections, should_exclude_first_section=True):\n",
        "    if self.is_strength_present and should_exclude_first_section:\n",
        "      self.first_section = sections[self.strength_idx]\n",
        "\n",
        "    in_between_sections = sections[self.strength_idx:self.discussion_idx] \n",
        "\n",
        "    if self.is_discussion_present:\n",
        "      self.last_section = sections[self.discussion_idx]\n",
        "\n",
        "    if self.is_weakness_present:\n",
        "      self.init_sections = sections[self.strength_idx:self.weakness_idx+1]\n",
        "      in_between_sections = sections[self.weakness_idx+1:self.discussion_idx] \n",
        "      \n",
        "    self.relevant_sections = self.init_sections\n",
        "\n",
        "    for section in in_between_sections:\n",
        "      section_strip_lower = section.strip().lower()\n",
        "      is_section_irrelevant = [keyword in section_strip_lower for keyword in self.irrelevant_section_keywords]\n",
        "      if any(is_section_irrelevant):\n",
        "        self.last_section = section\n",
        "        break  \n",
        "      self.relevant_sections.append(section)\n",
        "  \n",
        "  def merge_relevant_sections(self, sections, review_text, should_limit_strength_weak_section_len=False, should_move_weak_section_first=False):\n",
        "    if should_limit_strength_weak_section_len and self.is_weakness_present:\n",
        "      return self.limit_strength_weak_section_len(sections, review_text, should_move_weak_section_first)\n",
        "    else:\n",
        "      start_idx = review_text.find(self.first_section) if self.first_section else 0\n",
        "      end_idx = review_text.find(self.last_section)  if self.last_section else None\n",
        "\n",
        "      return review_text[start_idx:end_idx]\n",
        "      \n",
        "  def limit_strength_weak_section_len(self, sections, review_text, should_move_weak_section_first):\n",
        "      \n",
        "    strength_start_idx = review_text.find(self.first_section) if self.first_section else 0\n",
        "    strength_end_idx = review_text.find(sections[self.weakness_idx])\n",
        "\n",
        "    weak_end_idx = review_text.find(self.last_section) if self.last_section else None\n",
        "\n",
        "    strength_section_text = review_text[strength_start_idx:strength_end_idx]\n",
        "    weak_section_text = review_text[strength_end_idx:weak_end_idx]\n",
        "\n",
        "    strength_section_split = strength_section_text.split(\" \")\n",
        "    if len(strength_section_split) > self.section_len:\n",
        "      strength_section_split = strength_section_split[:self.section_len]\n",
        "      strength_section_text = \" \".join(strength_section_split)\n",
        "\n",
        "    weak_section_split = weak_section_text.split(\" \")\n",
        "    if len(weak_section_split) > self.section_len:\n",
        "      weak_section_split = weak_section_split[:self.section_len]\n",
        "      weak_section_text = \" \".join(weak_section_split)\n",
        "\n",
        "    if should_move_weak_section_first:\n",
        "      return weak_section_text + strength_section_text\n",
        "      \n",
        "    return strength_section_text + weak_section_text\n",
        "      \n",
        "def review_pruning(text_column):\n",
        "  section_detector_tokenizer = RegexpTokenizer('\\n\\n.{6,45}?:')\n",
        "  section_detector_relaxed_tokenizer = RegexpTokenizer('\\n\\n.{6,45}?\\n\\n?')\n",
        "\n",
        "  patterned_review_count = 0\n",
        "  unpatterned_review_count = 0\n",
        "  \n",
        "  patterned_review_idx = []\n",
        "\n",
        "  review_text_relevant_sections = []\n",
        "\n",
        "  for i in range(len(text_column)):\n",
        "    \n",
        "    review_text = text_column[i]\n",
        "    review_section = Review_Section()\n",
        "    \n",
        "    sections = section_detector_tokenizer.tokenize(review_text)\n",
        "    if len(sections):\n",
        "      review_section.try_detect_strength_weak_sections(sections)\n",
        "\n",
        "    if not review_section.any_strength_weak_sections:\n",
        "      sections = section_detector_relaxed_tokenizer.tokenize(review_text)\n",
        "      review_section.try_detect_strength_weak_sections(sections)\n",
        "\n",
        "    if review_section.any_strength_weak_sections:\n",
        "      patterned_review_idx.append(i)\n",
        "      patterned_review_count += 1\n",
        "    else:\n",
        "      unpatterned_review_count += 1\n",
        "\n",
        "    if len(sections):\n",
        "      review_section.compute_strength_weak_section(sections)\n",
        "#       review_section.compute_summary_section(sections)\n",
        "#       review_section.compute_weakness_section(sections)\n",
        "#       review_section.compute_strength_section(sections)\n",
        "#       review_section.compute_strength_weak_section(sections, should_exclude_first_section=False)\n",
        "      \n",
        "#     review_text = review_section.merge_relevant_sections(sections, review_text)\n",
        "#     review_text = review_section.merge_relevant_sections(sections, review_text, should_limit_strength_weak_section_len=True)\n",
        "    review_text = review_section.merge_relevant_sections(sections, review_text, should_limit_strength_weak_section_len=True,  should_move_weak_section_first=True)\n",
        "    \n",
        "    review_text_relevant_sections.append(review_text)\n",
        "\n",
        "  print(patterned_review_count)\n",
        "  print(unpatterned_review_count)\n",
        "  print(len(text_column))\n",
        "  \n",
        "  return review_text_relevant_sections, patterned_review_idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuR730_xr59h",
        "colab_type": "code",
        "outputId": "9c407b00-82ea-40f4-bc64-38e1c8020f5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "current_file = env_var.test_dataset_filename\n",
        "ds_df = pd.read_csv(env_var.dataset_path / current_file)\n",
        "ds_relevant_sections, patterned_review_idx = review_pruning(ds_df[\"comments\"])\n",
        "ds_df[\"comments\"] = ds_relevant_sections\n",
        "## ds_df = ds_df.iloc[patterned_review_idx]\n",
        "# ds_df.head()\n",
        "# ds_df.to_csv(env_var.dataset_path / \"weak_strength_sections_len_limit\" / current_file, index=False) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18\n",
            "9\n",
            "27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_fqJKWSzXLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copy strength_weak_sections_len_limit ds splits\n",
        "\n",
        "for i in range(10):\n",
        "  filename = \"train_ds_\" + str(i) + \".csv\" \n",
        "  \n",
        "  path_with_filename = env_var.dataset_path / \"manual_labeled_strength_weak_sections\" / \"aug_stren_weak_sections_len_limit\" / filename\n",
        "\n",
        "  strength_weak_sections_df = pd.read_csv(path_with_filename)\n",
        "\n",
        "  to_aug_ds_part = pd.read_csv(path_with_filename)\n",
        "  ds_relevant_sections, patterned_review_idx = review_pruning(to_aug_ds_part[\"comments\"])\n",
        "  to_aug_ds_part[\"comments\"] = ds_relevant_sections\n",
        "\n",
        "  strength_weak_sections_aug_df = pd.concat([strength_weak_sections_df, to_aug_ds_part])\n",
        "  aug_train_ds = strength_weak_sections_aug_df.drop_duplicates()\n",
        "#   aug_train_ds.to_csv(path_with_filename, index=False) \n",
        "  print(len(aug_train_ds))\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCHVANHx8FLu",
        "colab_type": "text"
      },
      "source": [
        "#### Clarity sentence manipulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoD4wWh4DiN_",
        "colab_type": "code",
        "outputId": "0c1d8daf-b750-417c-c067-5942747a3963",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def get_clarity_review_text_column(text_column, clarity_presence_regex_detection_list):\n",
        "  clarity_coverage = 0\n",
        "  clarity_review_text_column = []\n",
        "  indices = []\n",
        "  for i in range(len(text_column)):\n",
        "    review_text = text_column[i]\n",
        "    review_text_sentences = sent_tokenize(review_text)\n",
        "    clarity_sentences = []\n",
        "    is_clarity_detected_in_review = False\n",
        "    \n",
        "    for sentence in review_text_sentences:\n",
        "      normalize_sentence = sentence.lower().strip().replace(\"\\n\", \" \")\n",
        "      is_clarity_detected = any(re.match(regex, normalize_sentence) for regex in clarity_presence_regex_detection_list)\n",
        "      if is_clarity_detected:\n",
        "        clarity_sentences.append(sentence)\n",
        "        is_clarity_detected_in_review = True\n",
        "        \n",
        "    if is_clarity_detected_in_review:\n",
        "      clarity_review_text_column.append(\" \".join(clarity_sentences))\n",
        "      clarity_coverage += 1\n",
        "      indices.append(i)\n",
        "    else:\n",
        "      clarity_review_text_column.append(review_text)\n",
        "      \n",
        "  print(clarity_coverage, \"/\", len(text_column), \"/\", clarity_coverage / len(text_column))\n",
        "  return indices, clarity_review_text_column\n",
        "  \n",
        "paper_is_adjective = re.compile(\".*?paper +is +(?:\\w+ +)?(clear|unclear|thoroughly|extremely|easy|well|nice|nicely|clearly|poorly|\\w+ly).*?\")\n",
        "positive_written = re.compile(\".*?(nicely|well).*?written.*?\")\n",
        "clarity_keywords = re.compile(\".*?(typo.? +|understandable|thoroughly|structured|thorough|explanation|organized|misleading|mistakes|orthographical|grammar|spelling|readability|cohesion|explained|reads|written|clarity|clear|clearly).*?\")\n",
        "adjective_described = re.compile(\".*?(poorly|well|thoroughly|nicely|clearly|\\w+ly) +described.*?\")\n",
        "adjective_to_read = re.compile(\".*?(hard|easy) +to +read.*?\")\n",
        "language_error = re.compile(\".*?language +error.*?\")\n",
        "exists_paper_read = re.compile(\".*?(?=.*?paper)(?=.*?read).*\")\n",
        "native_speaker = re.compile(\".*?native +speaker.*?\")\n",
        "adjective_to_understand = re.compile(\".*?(hard|fail|difficult) +to +(understand|interpret).*?\")\n",
        "\n",
        "clarity_presence_regex_detection_list = [paper_is_adjective, positive_written, clarity_keywords, adjective_described, adjective_to_read, language_error, exists_paper_read, native_speaker, adjective_to_understand]\n",
        "\n",
        "current_file = env_var.test_dataset_filename\n",
        "aug_train_ds = pd.read_csv(env_var.dataset_path / current_file)\n",
        "indices, clarity_review_text_column = get_clarity_review_text_column(aug_train_ds[\"comments\"], clarity_presence_regex_detection_list)\n",
        "aug_train_ds[\"comments\"] = clarity_review_text_column\n",
        "# aug_train_ds\n",
        "# aug_train_ds.iloc[indices].to_csv(env_var.dataset_path / \"clarity_sentences/mandatory_clarity_sentences\" / current_file, index=False) \n",
        "# aug_train_ds.to_csv(env_var.dataset_path / \"clarity_sentences\" / current_file, index=False) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "210 / 269 / 0.7806691449814126\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjKM-hSLU8qp",
        "colab_type": "text"
      },
      "source": [
        "### Data preprocesssing, torchtext "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MCvG0TRU8qt",
        "colab_type": "text"
      },
      "source": [
        "#### TorchText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6H4mrbyU8qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext import data\n",
        "\n",
        "lower=True\n",
        "only_char=True\n",
        "stop_remove=False\n",
        "\n",
        "def tokenizer(review_text):\n",
        "    if lower: \n",
        "        review_text = review_text.lower()\n",
        "    if only_char:\n",
        "        regex_tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
        "        tokens = regex_tokenizer.tokenize(review_text)\n",
        "        review_text = ' '.join(tokens)\n",
        "    tokens = word_tokenize(review_text)\n",
        "    if stop_remove:\n",
        "        tokens = [w for w in tokens if not w in nltk.corpus.stopwords.words('english')]\n",
        "  \n",
        "    return tokens\n",
        "\n",
        "def get_tabular_datasets(seq_length):\n",
        "    comments_field = data.Field(sequential=True,\n",
        "                                tokenize=tokenizer,\n",
        "                                include_lengths=True,\n",
        "                                use_vocab=True,\n",
        "                                fix_length=seq_length)\n",
        "\n",
        "    grade_field = data.Field(sequential=False, \n",
        "                             use_vocab=False, \n",
        "                             pad_token=None,\n",
        "                             unk_token=None)\n",
        "    \n",
        "    dataset_fields = [\n",
        "        ('comments', comments_field), # process it as text\n",
        "        ('RECOMMENDATION', grade_field), # process it as label\n",
        "        ('REVIEWER_CONFIDENCE', grade_field),\n",
        "        ('SOUNDNESS_CORRECTNESS', grade_field),\n",
        "        ('IMPACT', grade_field),\n",
        "        ('SUBSTANCE', grade_field),\n",
        "        ('CLARITY', grade_field),\n",
        "        ('ORIGINALITY', grade_field)\n",
        "    ]\n",
        "\n",
        "    tabular_train, tabular_dev, tabular_test = data.TabularDataset.splits(path=env_var.dataset_path,\n",
        "                                                                          format='csv', \n",
        "                                                                          train=env_var.train_dataset_filename, \n",
        "                                                                          validation=env_var.dev_dataset_filename, \n",
        "                                                                          test=env_var.test_dataset_filename,\n",
        "                                                                          fields=dataset_fields, \n",
        "                                                                          skip_header=True)\n",
        "    return tabular_train, tabular_dev, tabular_test, comments_field\n",
        "\n",
        "          \n",
        "def get_vocab(comments_field, \n",
        "              tabular_train,\n",
        "              vocab_max_size,\n",
        "              vocab_min_freq,\n",
        "              pretrained_vectors,\n",
        "              mean=0, \n",
        "              std=0.01):\n",
        "    \n",
        "    unk_init_normal_ = lambda input_tensor: input_tensor.normal_(mean=mean, std=std)\n",
        "    \n",
        "    comments_field.build_vocab(\n",
        "        tabular_train,\n",
        "        max_size=vocab_max_size,\n",
        "        min_freq=vocab_min_freq,\n",
        "        vectors=pretrained_vectors,\n",
        "        unk_init=unk_init_normal_)\n",
        "    \n",
        "    return comments_field.vocab\n",
        "\n",
        "def get_batches_iterators(tabular_train, tabular_dev, tabular_test, \n",
        "                          comments_field_name,\n",
        "                          train_batch_size,\n",
        "                          dev_batch_size,\n",
        "                          test_batch_size):\n",
        "    # CREATE BATCH ITERATORS\n",
        "\n",
        "    train_batch, dev_batch = data.BucketIterator.splits(datasets=(tabular_train, tabular_dev), # specify train and validation Tabulardataset\n",
        "                                                        batch_sizes=(train_batch_size, dev_batch_size),  # batch size of train and validation\n",
        "                                                        sort_key=lambda x: len(x.comments), # on what attribute the text should be sorted\n",
        "                                                        sort_within_batch=True,\n",
        "                                                        repeat=False)\n",
        "\n",
        "    test_batch = data.Iterator(tabular_test, batch_size=test_batch_size, sort=False, sort_within_batch=False, repeat=False)\n",
        "\n",
        "    # BATCH DECORATOR \n",
        "\n",
        "    class BatchWrapper:\n",
        "        def __init__(self, dataset_batch, x_field):\n",
        "            self.dataset_batch, self.x_field = dataset_batch, x_field\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.dataset_batch)\n",
        "\n",
        "        def __iter__(self):\n",
        "            for batch in self.dataset_batch:\n",
        "                X, batch_seq_len = getattr(batch, self.x_field)\n",
        "    #             because tensorflow expects this tranposing\n",
        "                X = X.transpose(0,1).numpy()\n",
        "                batch_seq_len = batch_seq_len.numpy()\n",
        "                y = {}\n",
        "                for aspect in env_var.aspects_no_com_approp:\n",
        "                  y[aspect] = getattr(batch, aspect).numpy()\n",
        "                yield (X,y,batch_seq_len)\n",
        "\n",
        "    train_batch_it = BatchWrapper(train_batch, comments_field_name) \n",
        "    dev_batch_it = BatchWrapper(dev_batch, comments_field_name) \n",
        "    test_batch_it = BatchWrapper(test_batch, comments_field_name) \n",
        "    \n",
        "    return train_batch_it, dev_batch_it, test_batch_it"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4FjygDN_cr9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing_pipeline(seq_length, vocab_max_size, vocab_min_freq, pretrained_vectors, comments_field_name):\n",
        "  tabular_train, tabular_dev, tabular_test, comments_field = get_tabular_datasets(seq_length=seq_length)\n",
        "  vocab = get_vocab(comments_field, tabular_train, vocab_max_size=vocab_max_size, vocab_min_freq=vocab_min_freq, pretrained_vectors=pretrained_vectors)\n",
        "  \n",
        "  train_batch_size = len(tabular_train)\n",
        "  dev_batch_size = len(tabular_dev)\n",
        "  test_batch_size = len(tabular_test)\n",
        "\n",
        "  train_batch_it, dev_batch_it, test_batch_it = get_batches_iterators(tabular_train, tabular_dev, tabular_test, \n",
        "                                                                      comments_field_name=comments_field_name,\n",
        "                                                                      train_batch_size=train_batch_size,\n",
        "                                                                      dev_batch_size=dev_batch_size,\n",
        "                                                                      test_batch_size=test_batch_size)\n",
        "  embedding_dim = vocab.vectors.shape[1]\n",
        "  vocab_size = len(vocab)\n",
        "  return vocab, vocab_size, embedding_dim, train_batch_it, dev_batch_it, test_batch_it"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGa05TjVTWLg",
        "colab_type": "text"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFVAurNITejj",
        "colab_type": "text"
      },
      "source": [
        "### Helper base class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhXT78mPU8q_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_last_output(outputs, seq_length=None):\n",
        "    nr_batch_instances = tf.shape(outputs)[0]\n",
        "    batch_range = tf.range(nr_batch_instances)\n",
        "    last_idx_array = - tf.ones([nr_batch_instances],tf.int32) + tf.shape(outputs)[1]\n",
        "    \n",
        "    if seq_length is not None:\n",
        "        last_idx_array = seq_length - 1\n",
        "    \n",
        "    batch_range = tf.stack([batch_range,last_idx_array], axis=1)\n",
        "    \n",
        "    return tf.gather_nd(outputs, batch_range)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cem8I1zdwrNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN_base_model(object):\n",
        "  def __init__(self):\n",
        "    self.batch_seq_len = tf.placeholder(tf.int32, [None], name=\"seq_len\")\n",
        "    self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "    self.keep_prob_input = tf.placeholder(tf.float32, name='keep_prob_input')\n",
        "    self.keep_prob_cell = tf.placeholder(tf.float32, name='keep_prob_cell')\n",
        "    self.is_training = tf.placeholder(tf.bool, name=\"is_training\");\n",
        "    \n",
        "    self.dropout_layer = partial(tf.contrib.layers.dropout, keep_prob=self.keep_prob)    \n",
        "    self.input_dropout_layer = partial(tf.contrib.layers.dropout, keep_prob=self.keep_prob_input)    \n",
        "    self.batch_norm_layer = partial(tf.layers.batch_normalization, training=self.is_training, momentum=0.9)\n",
        "    self.fully_connected = partial(tf.contrib.layers.fully_connected, activation_fn=None)\n",
        "    \n",
        "    self.metrics_scope_name = \"metrics\"\n",
        "    \n",
        "  def embedding_layer_init(self, vocab_size, embedding_dim): \n",
        "    \n",
        "    self.embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim], name=\"embedding_placeholder\")\n",
        "\n",
        "    embedding = tf.get_variable('embedding', [vocab_size, embedding_dim], dtype=tf.float32, trainable=True)\n",
        "\n",
        "    self.embedding_init = embedding.assign(self.embedding_placeholder)\n",
        "\n",
        "    return embedding\n",
        "    \n",
        "  def rnn_encoder_layer(self, embedding_inputs, n_neurons, n_layers, cell_type, batch_seq_len, use_bidirectional):    \n",
        "    def cell_with_droput(layer_nr, name_suffix=\"\"):\n",
        "      cell_name = name_suffix + \"cell_\" + cell_type + \"_\" + str(layer_nr) \n",
        "      cell = self.get_cell(n_neurons, cell_type, cell_name)\n",
        "      \n",
        "      dropout_keep_prob = 1.0 if layer_nr == 0 else self.keep_prob_cell\n",
        "      \n",
        "      return tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=dropout_keep_prob)\n",
        "\n",
        "    if use_bidirectional:\n",
        "      with tf.name_scope(\"bi_rnn_layer\"):\n",
        "\n",
        "        cell_fw = tf.contrib.rnn.MultiRNNCell([cell_with_droput(i, \"fw\") for i in range(n_layers)])    \n",
        "        cell_bw = tf.contrib.rnn.MultiRNNCell([cell_with_droput(i, \"bw\") for i in range(n_layers)])    \n",
        "\n",
        "        (fw_pass, bw_pass), states = tf.nn.bidirectional_dynamic_rnn(cell_fw=cell_fw, cell_bw=cell_bw, inputs=embedding_inputs, dtype=tf.float32, sequence_length=batch_seq_len)\n",
        "\n",
        "        outputs = tf.concat([get_last_output(fw_pass, batch_seq_len), get_last_output(bw_pass, batch_seq_len)], 1)\n",
        "    else:\n",
        "      with tf.name_scope(\"rnn_layer\"):\n",
        "        cell = tf.contrib.rnn.MultiRNNCell([cell_with_droput(i) for i in range(n_layers)])    \n",
        "\n",
        "        outputs, states = tf.nn.dynamic_rnn(cell, embedding_inputs, dtype=tf.float32, sequence_length=batch_seq_len)\n",
        "        outputs = get_last_output(outputs, batch_seq_len)\n",
        "      \n",
        "    return outputs\n",
        "  \n",
        "  def optimize_loss(self, loss, learning_rate, weight_decay):\n",
        "#     optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "    optimizer = tf.contrib.opt.AdamWOptimizer(weight_decay=weight_decay, learning_rate=learning_rate)\n",
        "    self.training_op = optimizer.minimize(loss)\n",
        "  \n",
        "  def compute_list_mean(self, input_list):\n",
        "    return tf.reduce_mean(tf.convert_to_tensor(input_list, dtype=tf.float32))\n",
        "  \n",
        "  def get_loss(self, labels, logits):\n",
        "    return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
        "  \n",
        "  def get_accuracy(self, labels, logits):\n",
        "#     self.rmsqe = tf.sqrt(tf.losses.mean_squared_error(tf.cast(self.y_impact, tf.int64), tf.argmax(input=logits, axis=1)))\n",
        "    correct = tf.nn.in_top_k(logits, labels, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "    return accuracy\n",
        "  \n",
        "  def get_epoch_accuracy(self, labels, logits):\n",
        "    return tf.metrics.accuracy(labels=tf.cast(labels, tf.int64), predictions=tf.argmax(input=logits, axis=1))\n",
        "  \n",
        "  def get_epoch_conf_matrix(self, labels, logits):\n",
        "    return _streaming_confusion_matrix(tf.cast(labels, tf.int32), tf.argmax(input=logits, axis=1), 5)\n",
        "  \n",
        "  def register_metrics_init_op(self):\n",
        "    metric_variables = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope=self.metrics_scope_name)\n",
        "    self.metrics_init_op = tf.variables_initializer(metric_variables)\n",
        "  \n",
        "  def get_cell(self, n_neurons, cell_type, cell_name):\n",
        "    if cell_type == \"rnn\":\n",
        "      return tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, name=cell_name)\n",
        "    \n",
        "    if cell_type == \"gru\":\n",
        "      return tf.contrib.rnn.GRUCell(num_units=n_neurons, name=cell_name)\n",
        "\n",
        "    if cell_type == \"lstm\":\n",
        "      return tf.contrib.rnn.LSTMCell(num_units=n_neurons, name=cell_name)\n",
        "\n",
        "    if cell_type == \"lstm_batch_norm\":\n",
        "      return tf.contrib.rnn.LayerNormBasicLSTMCell(num_units=n_neurons)\n",
        "    \n",
        "    if cell_type == \"attention\":\n",
        "      cell = tf.contrib.rnn.GRUCell(num_units=n_neurons, name=cell_name)\n",
        "      return tf.contrib.rnn.AttentionCellWrapper(cell, 3)\n",
        "    \n",
        "    return tf.contrib.rnn.LSTMCell(num_units=n_neurons, use_peepholes=True, name=cell_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QT2WgIZTkx7",
        "colab_type": "text"
      },
      "source": [
        "### RNN single aspect learning model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eKOCU7FHE8tL",
        "colab": {}
      },
      "source": [
        "class RNN_single_aspect_model(RNN_base_model):\n",
        "  def __init__(self, model_config, vocab_size, embedding_dim, n_steps):\n",
        "    with tf.device(\"/gpu:0\"):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.X = tf.placeholder(tf.int32, [None, n_steps], name=\"X\")\n",
        "        self.y = tf.placeholder(tf.int32, [None], name=\"y\")\n",
        "        \n",
        "        embedding = self.embedding_layer_init(vocab_size, embedding_dim)       \n",
        "    \n",
        "        X_embeddings = tf.nn.embedding_lookup(embedding, self.X)\n",
        "      \n",
        "        X_embeddings = self.input_dropout_layer(X_embeddings)\n",
        "\n",
        "        outputs = self.rnn_encoder_layer(X_embeddings, model_config.n_neurons, model_config.n_layers, model_config.cell_type, self.batch_seq_len, model_config.use_bidirectional)\n",
        "        \n",
        "        outputs = self.batch_norm_layer(outputs)\n",
        "        \n",
        "        if model_config.use_extra_dense_layer:\n",
        "          outputs = self.dropout_layer(outputs)\n",
        "          outputs = self.fully_connected(outputs, model_config.n_neurons)\n",
        "          outputs = self.batch_norm_layer(outputs)\n",
        "          outputs = tf.nn.relu(outputs)\n",
        "      \n",
        "        outputs = self.dropout_layer(outputs)\n",
        "        logits = self.fully_connected(outputs, model_config.n_outputs)\n",
        "\n",
        "        self.batch_loss = self.get_loss(self.y, logits)\n",
        "    \n",
        "        self.optimize_loss(self.batch_loss, model_config.learning_rate, model_config.weight_decay)\n",
        "        \n",
        "        self.batch_accuracy = self.get_accuracy(self.y, logits)\n",
        "        \n",
        "        with tf.variable_scope(self.metrics_scope_name):\n",
        "          self.accuracy, accuracy_update_op = self.get_epoch_accuracy(self.y, logits)\n",
        "          self.loss, loss_update_op = tf.metrics.mean(self.batch_loss)\n",
        "          self.conf_matrix, conf_matrix_update_op = self.get_epoch_conf_matrix(self.y, logits)\n",
        "          \n",
        "        self.update_metrics_op = tf.group([accuracy_update_op, loss_update_op, conf_matrix_update_op])\n",
        "        \n",
        "        self.register_metrics_init_op()\n",
        "        \n",
        "        tf.summary.scalar(\"accuracy\", self.accuracy)\n",
        "        tf.summary.scalar(\"loss\", self.loss)\n",
        "        \n",
        "        self.summary_op = tf.summary.merge_all()\n",
        "\n",
        "        self.init = tf.global_variables_initializer()\n",
        "\n",
        "  def feed_dict_constructor(self, batch_comments, batch_label, batch_seq_len, droput_keep_prob, droput_keep_prob_input, droput_keep_prob_cell, is_training, aspect):\n",
        "    return {self.X: batch_comments, \n",
        "            self.y: batch_label[aspect] - 1,\n",
        "            self.batch_seq_len: batch_seq_len,\n",
        "            self.keep_prob:droput_keep_prob,\n",
        "            self.keep_prob_input: droput_keep_prob_input,\n",
        "            self.keep_prob_cell: droput_keep_prob_cell,\n",
        "            self.is_training: is_training}\n",
        "  \n",
        "  def perform_train_op(self, sess, train_batch_item, droput_keep_prob, droput_keep_prob_input, droput_keep_prob_cell, aspect):\n",
        "    sess.run(self.training_op, feed_dict=self.feed_dict_constructor(*train_batch_item, droput_keep_prob, droput_keep_prob_input, droput_keep_prob_cell, True, aspect))\n",
        "  \n",
        "  def update_batch_metrics(self, sess, batch_item, droput_keep_prob, aspect):\n",
        "    sess.run(self.update_metrics_op, feed_dict=self.feed_dict_constructor(*batch_item, droput_keep_prob, droput_keep_prob, droput_keep_prob, False, aspect))\n",
        "    \n",
        "  def get_epoch_metrics(self, sess, aspect):\n",
        "    metric_op = [self.conf_matrix, self.accuracy, self.loss, self.summary_op]\n",
        "    conf_matrix, accuracy, loss, summary = sess.run(metric_op)\n",
        "    return conf_matrix, accuracy, loss, summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V7ITr73TrkG",
        "colab_type": "text"
      },
      "source": [
        "### RNN multi task aspect learning model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1BQlgro0c_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN_multi_task_model(RNN_base_model):\n",
        "  def __init__(self, model_config, vocab_size, embedding_dim, n_steps):\n",
        "    with tf.device(\"/gpu:0\"):\n",
        "        super().__init__()\n",
        "        \n",
        "        n_neurons = model_config.n_neurons\n",
        "        n_outputs = model_config.n_outputs\n",
        "        \n",
        "        embedding = self.embedding_layer_init(vocab_size, embedding_dim)\n",
        "        \n",
        "        self.X = tf.placeholder(tf.int32, [None, n_steps], name=\"X\")\n",
        "    \n",
        "        X_embeddings = tf.nn.embedding_lookup(embedding, self.X)\n",
        "        \n",
        "        X_embeddings = self.input_dropout_layer(X_embeddings)\n",
        "        \n",
        "        self.y_rec = tf.placeholder(tf.int32, [None], name=\"y_rec\")\n",
        "        self.y_impact = tf.placeholder(tf.int32, [None], name=\"y_impact\")\n",
        "        self.y_substance = tf.placeholder(tf.int32, [None], name=\"y_substance\")\n",
        "        self.y_clarity = tf.placeholder(tf.int32, [None], name=\"y_clarity\")\n",
        "        self.y_confidence = tf.placeholder(tf.int32, [None], name=\"y_confidence\")\n",
        "        self.y_correctness = tf.placeholder(tf.int32, [None], name=\"y_correctness\")\n",
        "        self.y_originality = tf.placeholder(tf.int32, [None], name=\"y_originality\")\n",
        "        \n",
        "        outputs = self.rnn_encoder_layer(X_embeddings, n_neurons, model_config.n_layers, model_config.cell_type, self.batch_seq_len, model_config.use_bidirectional)\n",
        "\n",
        "        if model_config.use_extra_dense_layer:\n",
        "          outputs = self.bn_drop_liner(outputs, n_neurons)\n",
        "        \n",
        "        rec_dense_layer = self.bn_drop_liner(outputs, n_neurons)\n",
        "        impact_dense_layer = self.bn_drop_liner(outputs, n_neurons)\n",
        "        substance_dense_layer = self.bn_drop_liner(outputs, n_neurons)\n",
        "        clarity_dense_layer = self.bn_drop_liner(outputs, n_neurons)\n",
        "        confidence_dense_layer = self.bn_drop_liner(outputs, n_neurons)\n",
        "        correctness_dense_layer = self.bn_drop_liner(outputs, n_neurons)\n",
        "        originality_dense_layer = self.bn_drop_liner(outputs, n_neurons)\n",
        "\n",
        "        rec_dense_layer = self.bn_drop_liner(rec_dense_layer, n_neurons)\n",
        "        impact_dense_layer = self.bn_drop_liner(impact_dense_layer, n_neurons)\n",
        "        substance_dense_layer = self.bn_drop_liner(substance_dense_layer, n_neurons)\n",
        "        clarity_dense_layer = self.bn_drop_liner(clarity_dense_layer, n_neurons)\n",
        "        confidence_dense_layer = self.bn_drop_liner(confidence_dense_layer, n_neurons)\n",
        "        correctness_dense_layer = self.bn_drop_liner(correctness_dense_layer, n_neurons)\n",
        "        originality_dense_layer = self.bn_drop_liner(originality_dense_layer, n_neurons)\n",
        "        \n",
        "        rec_logits = self.bn_drop_liner_no_activation(rec_dense_layer, n_outputs)\n",
        "        impact_logits = self.bn_drop_liner_no_activation(impact_dense_layer, n_outputs)\n",
        "        substance_logits = self.bn_drop_liner_no_activation(substance_dense_layer, n_outputs)\n",
        "        clarity_logits = self.bn_drop_liner_no_activation(clarity_dense_layer, n_outputs)\n",
        "        confidence_logits = self.bn_drop_liner_no_activation(confidence_dense_layer, n_outputs)\n",
        "        correctness_logits = self.bn_drop_liner_no_activation(correctness_dense_layer, n_outputs)\n",
        "        originality_logits = self.bn_drop_liner_no_activation(originality_dense_layer, n_outputs)\n",
        "\n",
        "        self.batch_rec_loss = self.get_loss(self.y_rec, rec_logits)\n",
        "        self.batch_impact_loss = self.get_loss(self.y_impact, impact_logits)\n",
        "        self.batch_substance_loss = self.get_loss(self.y_substance, substance_logits)\n",
        "        self.batch_clarity_loss = self.get_loss(self.y_clarity, clarity_logits)\n",
        "        self.batch_confidence_loss = self.get_loss(self.y_confidence, confidence_logits)\n",
        "        self.batch_correctness_loss = self.get_loss(self.y_correctness, correctness_logits)\n",
        "        self.batch_originality_loss = self.get_loss(self.y_originality, originality_logits)\n",
        "        \n",
        "        weighted_loss = tf.math.multiply([self.batch_rec_loss, self.batch_impact_loss, self.batch_substance_loss, self.batch_clarity_loss, self.batch_confidence_loss, self.batch_correctness_loss, self.batch_originality_loss], \n",
        "                                         model_config.aspect_weights)\n",
        "        \n",
        "        self.batch_loss = self.compute_list_mean(weighted_loss)\n",
        "        \n",
        "        self.optimize_loss(self.batch_loss, model_config.learning_rate, model_config.weight_decay)\n",
        "        \n",
        "        self.batch_rec_accuracy = self.get_accuracy(self.y_rec, rec_logits)\n",
        "        self.batch_impact_accuracy = self.get_accuracy(self.y_impact, impact_logits)\n",
        "        self.batch_substance_accuracy = self.get_accuracy(self.y_substance, substance_logits)\n",
        "        self.batch_clarity_accuracy = self.get_accuracy(self.y_clarity, clarity_logits)\n",
        "        self.batch_confidence_accuracy = self.get_accuracy(self.y_confidence, confidence_logits)\n",
        "        self.batch_correctness_accuracy = self.get_accuracy(self.y_correctness, correctness_logits)\n",
        "        self.batch_originality_accuracy = self.get_accuracy(self.y_originality, originality_logits)\n",
        "        \n",
        "        self.batch_accuracy = self.compute_list_mean([self.batch_impact_accuracy, self.batch_rec_accuracy, self.batch_substance_accuracy, self.batch_clarity_accuracy, self.batch_confidence_accuracy, self.batch_correctness_accuracy, self.batch_originality_accuracy])\n",
        "        \n",
        "        with tf.variable_scope(self.metrics_scope_name):\n",
        "          self.rec_accuracy, rec_accuracy_update_op = self.get_epoch_accuracy(self.y_rec, rec_logits)\n",
        "          self.impact_accuracy, impact_accuracy_update_op = self.get_epoch_accuracy(self.y_impact, impact_logits)\n",
        "          self.substance_accuracy, substance_accuracy_update_op = self.get_epoch_accuracy(self.y_substance, substance_logits)\n",
        "          self.clarity_accuracy, clarity_accuracy_update_op = self.get_epoch_accuracy(self.y_clarity, clarity_logits)\n",
        "          self.confidence_accuracy, confidence_accuracy_update_op = self.get_epoch_accuracy(self.y_confidence, confidence_logits)\n",
        "          self.correctness_accuracy, correctness_accuracy_update_op = self.get_epoch_accuracy(self.y_correctness, correctness_logits)\n",
        "          self.originality_accuracy, originality_accuracy_update_op = self.get_epoch_accuracy(self.y_originality, originality_logits)\n",
        "        \n",
        "          self.rec_loss, rec_loss_update_op = tf.metrics.mean(self.batch_rec_loss)\n",
        "          self.impact_loss, impact_loss_update_op = tf.metrics.mean(self.batch_impact_loss)\n",
        "          self.substance_loss, substance_loss_update_op = tf.metrics.mean(self.batch_substance_loss)\n",
        "          self.clarity_loss, clarity_loss_update_op = tf.metrics.mean(self.batch_clarity_loss)\n",
        "          self.confidence_loss, confidence_loss_update_op = tf.metrics.mean(self.batch_confidence_loss)\n",
        "          self.correctness_loss, correctness_loss_update_op = tf.metrics.mean(self.batch_correctness_loss)\n",
        "          self.originality_loss, originality_loss_update_op = tf.metrics.mean(self.batch_originality_loss)\n",
        "          \n",
        "          self.rec_conf_matrix, rec_conf_matrix_update_op = self.get_epoch_conf_matrix(self.y_rec, rec_logits)\n",
        "          self.impact_conf_matrix, impact_conf_matrix_update_op = self.get_epoch_conf_matrix(self.y_impact, impact_logits)\n",
        "          self.substance_conf_matrix, substance_conf_matrix_update_op = self.get_epoch_conf_matrix(self.y_substance, substance_logits)\n",
        "          self.clarity_conf_matrix, clarity_conf_matrix_update_op = self.get_epoch_conf_matrix(self.y_clarity, clarity_logits)\n",
        "          self.confidence_conf_matrix, confidence_conf_matrix_update_op = self.get_epoch_conf_matrix(self.y_confidence, confidence_logits)\n",
        "          self.correctness_conf_matrix, correctness_conf_matrix_update_op = self.get_epoch_conf_matrix(self.y_correctness, correctness_logits)\n",
        "          self.originality_conf_matrix, originality_conf_matrix_update_op = self.get_epoch_conf_matrix(self.y_originality, originality_logits)\n",
        "          \n",
        "        self.update_metrics_op = tf.group([rec_accuracy_update_op, impact_accuracy_update_op, substance_accuracy_update_op, clarity_accuracy_update_op, \n",
        "                                          confidence_accuracy_update_op, correctness_accuracy_update_op, originality_accuracy_update_op,\n",
        "                                          rec_loss_update_op, impact_loss_update_op, substance_loss_update_op, clarity_loss_update_op,\n",
        "                                          confidence_loss_update_op, correctness_loss_update_op, originality_loss_update_op, \n",
        "                                          rec_conf_matrix_update_op, impact_conf_matrix_update_op, substance_conf_matrix_update_op, \n",
        "                                          clarity_conf_matrix_update_op, confidence_conf_matrix_update_op, correctness_conf_matrix_update_op,\n",
        "                                          originality_conf_matrix_update_op])\n",
        "        \n",
        "        self.register_metrics_init_op()\n",
        "\n",
        "        tf.summary.scalar(\"accuracy/rec\", self.rec_accuracy)\n",
        "        tf.summary.scalar(\"accuracy/impact\", self.impact_accuracy)\n",
        "        tf.summary.scalar(\"accuracy/subst\", self.substance_accuracy)\n",
        "        tf.summary.scalar(\"accuracy/clarity\", self.clarity_accuracy)\n",
        "        tf.summary.scalar(\"accuracy/confidence\", self.confidence_accuracy)\n",
        "        tf.summary.scalar(\"accuracy/correctness\", self.correctness_accuracy)\n",
        "        tf.summary.scalar(\"accuracy/originality\", self.originality_accuracy)\n",
        "\n",
        "        tf.summary.scalar(\"loss/rec\", self.rec_loss)\n",
        "        tf.summary.scalar(\"loss/impact\", self.impact_loss)\n",
        "        tf.summary.scalar(\"loss/substance\", self.substance_loss)\n",
        "        tf.summary.scalar(\"loss/clarity\", self.clarity_loss)\n",
        "        tf.summary.scalar(\"loss/confidence\", self.confidence_loss)\n",
        "        tf.summary.scalar(\"loss/correctness\", self.correctness_loss)\n",
        "        tf.summary.scalar(\"loss/originality\", self.originality_loss)\n",
        "        \n",
        "        self.summary_op = tf.summary.merge_all()\n",
        "\n",
        "        self.init = tf.global_variables_initializer()\n",
        "        \n",
        "  def bn_drop_liner(self, inputs, nr_units):\n",
        "    outputs = self.batch_norm_layer(inputs)\n",
        "    outputs = self.dropout_layer(outputs)\n",
        "    outputs = fully_connected(outputs, nr_units)\n",
        "    return outputs\n",
        "\n",
        "  def bn_drop_liner_no_activation(self, inputs, nr_units):\n",
        "    outputs = self.batch_norm_layer(inputs)\n",
        "    outputs = self.dropout_layer(outputs)\n",
        "    outputs = fully_connected(outputs, nr_units, activation_fn=None)\n",
        "    return outputs\n",
        "  \n",
        "  def feed_dict_constructor(self, batch_comments, batch_label, batch_seq_len, droput_keep_prob, droput_keep_prob_input, droput_keep_prob_cell, is_training):\n",
        "    return {self.X: batch_comments, \n",
        "            self.y_rec: batch_label[\"RECOMMENDATION\"] - 1, \n",
        "            self.y_impact: batch_label[\"IMPACT\"] - 1, \n",
        "            self.y_substance: batch_label[\"SUBSTANCE\"] - 1,\n",
        "            self.y_clarity: batch_label[\"CLARITY\"] - 1,\n",
        "            self.y_confidence: batch_label[\"REVIEWER_CONFIDENCE\"] - 1,\n",
        "            self.y_correctness: batch_label[\"SOUNDNESS_CORRECTNESS\"] - 1,\n",
        "            self.y_originality: batch_label[\"ORIGINALITY\"] - 1,\n",
        "            self.batch_seq_len: batch_seq_len,\n",
        "            self.keep_prob:droput_keep_prob,\n",
        "            self.keep_prob_input: droput_keep_prob_input,\n",
        "            self.keep_prob_cell: droput_keep_prob_cell,\n",
        "            self.is_training: is_training}\n",
        "  \n",
        "  def perform_train_op(self, sess, train_batch_item, droput_keep_prob, droput_keep_prob_input, droput_keep_prob_cell, aspect):\n",
        "    sess.run(self.training_op, feed_dict=self.feed_dict_constructor(*train_batch_item, droput_keep_prob, droput_keep_prob_input, droput_keep_prob_cell, True))\n",
        "    \n",
        "  def update_batch_metrics(self, sess, batch_item, droput_keep_prob, aspect):\n",
        "    sess.run(self.update_metrics_op, feed_dict=self.feed_dict_constructor(*batch_item, droput_keep_prob, droput_keep_prob, droput_keep_prob, False))\n",
        "  \n",
        "  def get_epoch_metrics(self, sess, aspect):\n",
        "    aspect_ops_dict = {\"RECOMMENDATION\": [self.rec_conf_matrix, self.rec_accuracy, self.rec_loss],\n",
        "                       \"IMPACT\": [self.impact_conf_matrix, self.impact_accuracy, self.impact_loss],\n",
        "                       \"SUBSTANCE\": [self.substance_conf_matrix, self.substance_accuracy, self.substance_loss],\n",
        "                       \"CLARITY\": [self.clarity_conf_matrix, self.clarity_accuracy, self.clarity_loss],\n",
        "                       \"REVIEWER_CONFIDENCE\": [self.confidence_conf_matrix, self.confidence_accuracy, self.confidence_loss],\n",
        "                         \"SOUNDNESS_CORRECTNESS\": [self.correctness_conf_matrix, self.correctness_accuracy, self.correctness_loss],\n",
        "                       \"ORIGINALITY\": [self.originality_conf_matrix, self.originality_accuracy, self.originality_loss]}\n",
        "          \n",
        "    metric_op = aspect_ops_dict[str(aspect)] + [self.summary_op]\n",
        "\n",
        "    conf_matrix, accuracy, train_loss, summary = sess.run(metric_op)\n",
        "    return conf_matrix, accuracy, train_loss, summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-X_rLBKTwzb",
        "colab_type": "text"
      },
      "source": [
        "### RNN conditional model (cond review on aspect)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P1j2Hr2f468",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN_conditional_model(RNN_base_model):\n",
        "  def __init__(self, model_config, vocab_size, embedding_dim, n_steps):\n",
        "    \n",
        "    self.aspect_vocab_indx = {\"RECOMMENDATION\": vocab.stoi[\"suggestion\"],\n",
        "                              \"IMPACT\": vocab.stoi[\"impact\"],\n",
        "                               \"SUBSTANCE\": vocab.stoi[\"significant\"],\n",
        "                               \"CLARITY\": vocab.stoi[\"clarity\"],\n",
        "                               \"REVIEWER_CONFIDENCE\": vocab.stoi[\"confident\"],\n",
        "                               \"SOUNDNESS_CORRECTNESS\": vocab.stoi[\"correct\"],\n",
        "                               \"ORIGINALITY\": vocab.stoi[\"originality\"]}\n",
        "    \n",
        "    with tf.device(\"/gpu:0\"):\n",
        "        super().__init__()\n",
        "        \n",
        "        embedding = self.embedding_layer_init(vocab_size, embedding_dim)\n",
        "        \n",
        "        self.aspect = tf.placeholder(tf.int32, [None, 1], name=\"aspect\")\n",
        "        \n",
        "        aspect_embeddings = tf.nn.embedding_lookup(embedding, self.aspect)\n",
        "        \n",
        "        def cell_with_droput(layer_nr, name_suffix=\"\"):\n",
        "          cell_name = name_suffix + \"cell_\" + model_config.cell_type + \"_\" + str(layer_nr) \n",
        "          cell = self.get_cell(model_config.n_neurons, model_config.cell_type, cell_name)\n",
        "\n",
        "          dropout_keep_prob = 1.0 if layer_nr == 0 else self.keep_prob\n",
        "\n",
        "          return tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=dropout_keep_prob)        \n",
        "        \n",
        "        def get_named_cell(layer_nr, name_suffix=\"\"):\n",
        "          cell_name = name_suffix + \"cell_\" + model_config.cell_type + \"_\" + str(layer_nr) \n",
        "          return self.get_cell(model_config.n_neurons, model_config.cell_type, cell_name)\n",
        "\n",
        "        cell_fw_aspect = tf.contrib.rnn.MultiRNNCell([get_named_cell(i, \"fw_aspect\") for i in range(1)])    \n",
        "        cell_bw_aspect = tf.contrib.rnn.MultiRNNCell([get_named_cell(i, \"bw_aspect\") for i in range(1)])\n",
        "        \n",
        "        initial_state_fw = cell_fw_aspect.zero_state(tf.shape(self.aspect)[0], dtype=tf.float32)\n",
        "        initial_state_bw = cell_bw_aspect.zero_state(tf.shape(self.aspect)[0], dtype=tf.float32)\n",
        "\n",
        "        (init_fw_pass, init_bw_pass), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=cell_fw_aspect, cell_bw=cell_bw_aspect, inputs=aspect_embeddings, dtype=tf.float32, initial_state_fw=initial_state_fw, initial_state_bw=initial_state_bw)\n",
        "        \n",
        "        init_fw_pass = get_last_output(init_fw_pass)\n",
        "        init_bw_pass = get_last_output(init_bw_pass)\n",
        "        \n",
        "        init_fw_pass = tuple([init_fw_pass for _ in range(model_config.n_layers)])\n",
        "        init_bw_pass = tuple([init_bw_pass for _ in range(model_config.n_layers)])\n",
        "        \n",
        "        self.X = tf.placeholder(tf.int32, [None, n_steps], name=\"X\")\n",
        "        self.y = tf.placeholder(tf.int32, [None], name=\"y\")\n",
        "\n",
        "        X_embeddings = tf.nn.embedding_lookup(embedding, self.X)\n",
        "        X_embeddings = self.input_dropout_layer(X_embeddings)\n",
        "        \n",
        "        cell_fw = tf.contrib.rnn.MultiRNNCell([cell_with_droput(i, \"fw\") for i in range(model_config.n_layers)])    \n",
        "        cell_bw = tf.contrib.rnn.MultiRNNCell([cell_with_droput(i, \"bw\") for i in range(model_config.n_layers)])\n",
        "        \n",
        "        (fw_pass, bw_pass), states = tf.nn.bidirectional_dynamic_rnn(cell_fw=cell_fw, cell_bw=cell_bw, inputs=X_embeddings, dtype=tf.float32, sequence_length=self.batch_seq_len, initial_state_fw=init_fw_pass, initial_state_bw=init_bw_pass)\n",
        "\n",
        "        outputs = tf.concat([get_last_output(fw_pass, self.batch_seq_len), get_last_output(bw_pass, self.batch_seq_len)], 1)\n",
        "        \n",
        "        if model_config.use_extra_dense_layer:\n",
        "          outputs = self.dropout_layer(outputs)\n",
        "          outputs = fully_connected(outputs, model_config.n_neurons)\n",
        "\n",
        "        outputs = self.dropout_layer(outputs)\n",
        "        outputs = fully_connected(outputs, model_config.n_neurons)  \n",
        "          \n",
        "        outputs = self.dropout_layer(outputs)\n",
        "        logits = fully_connected(outputs, model_config.n_outputs, activation_fn=None)\n",
        "\n",
        "        self.batch_loss = self.get_loss(self.y, logits)\n",
        "\n",
        "        self.optimize_loss(self.batch_loss, model_config.learning_rate, model_config.weight_decay)\n",
        "\n",
        "        self.batch_accuracy = self.get_accuracy(self.y, logits)\n",
        "        \n",
        "        with tf.variable_scope(self.metrics_scope_name):\n",
        "          self.accuracy, accuracy_update_op = self.get_epoch_accuracy(self.y, logits)\n",
        "          self.loss, loss_update_op = tf.metrics.mean(self.batch_loss)\n",
        "          self.conf_matrix, conf_matrix_update_op = self.get_epoch_conf_matrix(self.y, logits)\n",
        "  \n",
        "        self.update_metrics_op = tf.group([accuracy_update_op, loss_update_op, conf_matrix_update_op])\n",
        "        \n",
        "        self.register_metrics_init_op()\n",
        "\n",
        "        tf.summary.scalar(\"loss\", self.loss)\n",
        "        tf.summary.scalar(\"accuracy\", self.accuracy)\n",
        "\n",
        "        self.summary_op = tf.summary.merge_all()\n",
        "\n",
        "        self.init = tf.global_variables_initializer()\n",
        "        \n",
        "  def feed_dict_constructor(self, batch_comments, batch_label, batch_seq_len, droput_keep_prob, droput_keep_prob_input, droput_keep_prob_cell, is_training, aspect):\n",
        "    return {self.X: batch_comments, \n",
        "            self.y: batch_label[aspect] - 1,\n",
        "            self.batch_seq_len: batch_seq_len,\n",
        "            self.keep_prob:droput_keep_prob,\n",
        "            self.keep_prob_input: droput_keep_prob_input,\n",
        "            self.keep_prob_cell: droput_keep_prob_cell,\n",
        "            self.is_training: is_training,\n",
        "            self.aspect: [[self.aspect_vocab_indx[aspect]] for _ in range(len(batch_label[aspect]))]}\n",
        "  \n",
        "  def perform_train_op(self, sess, train_batch_item, droput_keep_prob, droput_keep_prob_input, droput_keep_prob_cell, sess_aspect):\n",
        "    for aspect in env_var.aspects_no_com_approp:\n",
        "      sess.run(self.training_op, feed_dict=self.feed_dict_constructor(*train_batch_item, droput_keep_prob, droput_keep_prob_input, droput_keep_prob_cell, True, aspect))\n",
        "  \n",
        "  \n",
        "  def update_batch_metrics(self, sess, batch_item, droput_keep_prob, aspect):\n",
        "    sess.run(self.update_metrics_op, feed_dict=self.feed_dict_constructor(*batch_item, droput_keep_prob, droput_keep_prob, droput_keep_prob, False, aspect))\n",
        "    \n",
        "  def get_epoch_metrics(self, sess, aspect):\n",
        "    metric_op = [self.conf_matrix, self.accuracy, self.loss, self.summary_op]\n",
        "    conf_matrix, accuracy, loss, summary = sess.run(metric_op)\n",
        "    return conf_matrix, accuracy, loss, summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRP5xc7Cz6Wb",
        "colab_type": "text"
      },
      "source": [
        "### CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtXWF0VDzPZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN(RNN_base_model):\n",
        "  def __init__(self, model_config, vocab_size, embedding_dim, n_steps):\n",
        "    with tf.device('/gpu:0'):\n",
        "      super().__init__()\n",
        "\n",
        "      self.X = tf.placeholder(tf.int32, [None, n_steps], name=\"X\")\n",
        "      self.y = tf.placeholder(tf.int32, [None], name=\"y\")\n",
        "\n",
        "      embedding = self.embedding_layer_init(vocab_size, embedding_dim)       \n",
        "\n",
        "      X_embeddings = tf.nn.embedding_lookup(embedding, self.X)\n",
        "\n",
        "      X_embeddings = self.input_dropout_layer(X_embeddings)\n",
        "\n",
        "      with tf.name_scope(\"cnn\"):\n",
        "        conv = tf.layers.conv1d(X_embeddings, model_config.num_filters, model_config.kernel_size)\n",
        "        conv = tf.layers.max_pooling1d(conv,3,2)\n",
        "        gmp = tf.reduce_max(conv, reduction_indices=[1])\n",
        "        \n",
        "      fc = tf.layers.dense(gmp, model_config.n_neurons)\n",
        "      fc = tf.contrib.layers.dropout(fc, model_config.droput_keep_prob)\n",
        "#       fc = self.batch_norm_layer(fc)\n",
        "      fc = tf.nn.relu(fc)\n",
        "      \n",
        "      fc = tf.layers.dense(fc, model_config.n_neurons)\n",
        "      fc = tf.contrib.layers.dropout(fc, model_config.droput_keep_prob)\n",
        "#       fc = self.batch_norm_layer(fc)\n",
        "      fc = tf.nn.relu(fc)\n",
        "      \n",
        "      logits = tf.layers.dense(fc, model_config.n_outputs)\n",
        "\n",
        "      self.batch_loss = self.get_loss(self.y, logits)\n",
        "\n",
        "      self.optimize_loss(self.batch_loss, model_config.learning_rate, model_config.weight_decay)\n",
        "\n",
        "      self.batch_accuracy = self.get_accuracy(self.y, logits)\n",
        "\n",
        "      with tf.variable_scope(self.metrics_scope_name):\n",
        "        self.accuracy, accuracy_update_op = self.get_epoch_accuracy(self.y, logits)\n",
        "        self.loss, loss_update_op = tf.metrics.mean(self.batch_loss)\n",
        "        self.conf_matrix, conf_matrix_update_op = self.get_epoch_conf_matrix(self.y, logits)\n",
        "\n",
        "      self.update_metrics_op = tf.group([accuracy_update_op, loss_update_op, conf_matrix_update_op])\n",
        "\n",
        "      self.register_metrics_init_op()\n",
        "\n",
        "      tf.summary.scalar(\"accuracy\", self.accuracy)\n",
        "      tf.summary.scalar(\"loss\", self.loss)\n",
        "\n",
        "      self.summary_op = tf.summary.merge_all()\n",
        "\n",
        "      self.init = tf.global_variables_initializer()\n",
        "\n",
        "  def feed_dict_constructor(self, batch_comments, batch_label, batch_seq_len, droput_keep_prob, droput_keep_prob_input, droput_keep_prob_cell, is_training, aspect):\n",
        "    return {self.X: batch_comments, \n",
        "            self.y: batch_label[aspect] - 1,\n",
        "            self.batch_seq_len: batch_seq_len,\n",
        "            self.keep_prob:droput_keep_prob,\n",
        "            self.keep_prob_input: droput_keep_prob_input,\n",
        "            self.keep_prob_cell: droput_keep_prob_cell,\n",
        "            self.is_training: is_training}\n",
        "\n",
        "  def perform_train_op(self, sess, train_batch_item, droput_keep_prob, droput_keep_prob_input, droput_keep_prob_cell, aspect):\n",
        "    sess.run(self.training_op, feed_dict=self.feed_dict_constructor(*train_batch_item, droput_keep_prob, droput_keep_prob_input, droput_keep_prob_cell, True, aspect))\n",
        "\n",
        "  def update_batch_metrics(self, sess, batch_item, droput_keep_prob, aspect):\n",
        "    sess.run(self.update_metrics_op, feed_dict=self.feed_dict_constructor(*batch_item, droput_keep_prob, droput_keep_prob, droput_keep_prob, False, aspect))\n",
        "\n",
        "  def get_epoch_metrics(self, sess, aspect):\n",
        "    metric_op = [self.conf_matrix, self.accuracy, self.loss, self.summary_op]\n",
        "    conf_matrix, accuracy, loss, summary = sess.run(metric_op)\n",
        "    return conf_matrix, accuracy, loss, summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oVbh3GCT_kP",
        "colab_type": "text"
      },
      "source": [
        "## Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMDqdweRozE_",
        "colab_type": "text"
      },
      "source": [
        "#### Helper funs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7uLDsbmTjIP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/ageron/handson-ml/blob/master/11_deep_learning.ipynb\n",
        "\n",
        "def _get_model_params(sess):\n",
        "    \"\"\"Get all variable values (used for early stopping, faster than saving to disk)\"\"\"\n",
        "    gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
        "    return {gvar.op.name: value for gvar, value in zip(gvars, sess.run(gvars))}\n",
        "\n",
        "def _restore_model_params(sess, model_params):\n",
        "    \"\"\"Set all variables to the given values (for early stopping, faster than loading from disk)\"\"\"\n",
        "    gvar_names = list(model_params.keys())\n",
        "    assign_ops = {gvar_name: tf.get_default_graph().get_operation_by_name(gvar_name + \"/Assign\") for gvar_name in gvar_names}\n",
        "    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
        "    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
        "    sess.run(assign_ops, feed_dict=feed_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esQHZE_tzOqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Epoch_Metrics():\n",
        "  def __init__(self, conf_matrix, accuracy, loss):\n",
        "    self.conf_matrix = conf_matrix\n",
        "    self.accuracy = accuracy\n",
        "    self.loss = loss\n",
        "    \n",
        "    \n",
        "class Stat():\n",
        "  def __init__(self, epoch, train_metrics, dev_metrics, test_metrics, iteration):\n",
        "    self.epoch = epoch    \n",
        "    self.train_accuracy = train_metrics.accuracy\n",
        "    self.dev_accuracy = dev_metrics.accuracy\n",
        "    self.train_loss = train_metrics.loss\n",
        "    self.dev_loss = dev_metrics.loss\n",
        "    self.test_accuracy = test_metrics.accuracy\n",
        "    self.dev_conf_matrix = dev_metrics.conf_matrix\n",
        "    self.test_conf_matrix = test_metrics.conf_matrix\n",
        "    self.iteration = iteration\n",
        "    \n",
        "  def __str__(self):\n",
        "    return f'{self.epoch} Train accuracy: {self.train_accuracy} Dev accuracy: {self.dev_accuracy} Train loss: {self.train_loss} Dev loss: {self.dev_loss}, Test acc: {self.test_accuracy}'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Azr6w_risCSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Train_Stat_Tracker():\n",
        "  def __init__(self):\n",
        "    \n",
        "    self.best_dev_accuracy = 0\n",
        "    self.best_dev_acc_epoch = 0\n",
        "    self.best_dev_loss_epoch = 0\n",
        "    self.best_dev_loss = np.infty\n",
        "    \n",
        "    self.best_params = None\n",
        "\n",
        "    self.checks_without_progress = 0\n",
        "    self.max_checks_without_progress = 10\n",
        "    \n",
        "    self.nr_epoch_neigh = 2\n",
        "    \n",
        "    self.train_stats = []\n",
        "    self.best_epoch_stats = None\n",
        "      \n",
        "  def record_train_stats(self, sess, epoch, train_metrics, dev_metrics, test_metrics, iteration):\n",
        "\n",
        "    epoch_stat = Stat(epoch, train_metrics, dev_metrics, test_metrics, iteration)\n",
        "\n",
        "    self.train_stats.append(epoch_stat)\n",
        "\n",
        "    if epoch_stat.train_loss < epoch_stat.dev_loss and epoch_stat.train_accuracy >= epoch_stat.dev_accuracy:\n",
        "\n",
        "      if self.best_dev_accuracy < epoch_stat.dev_accuracy:\n",
        "        self.best_dev_accuracy = epoch_stat.dev_accuracy\n",
        "        self.best_dev_acc_epoch = epoch\n",
        "\n",
        "      if self.best_dev_loss > epoch_stat.dev_loss:\n",
        "        self.best_dev_loss = epoch_stat.dev_loss\n",
        "        self.best_dev_loss_epoch = epoch\n",
        "#         self.best_params = _get_model_params(sess)   \n",
        "        self.checks_without_progress = 0\n",
        "      else:\n",
        "        self.checks_without_progress += 1\n",
        "    \n",
        "    return epoch_stat\n",
        "  \n",
        "  def check_early_stopping(self, is_last_epoch):\n",
        "    should_early_stop = False\n",
        "\n",
        "    if self.checks_without_progress > self.max_checks_without_progress or is_last_epoch:\n",
        "      \n",
        "      left_window = self.best_dev_loss_epoch - self.nr_epoch_neigh\n",
        "\n",
        "      if left_window < 0: \n",
        "        left_window = 0\n",
        "\n",
        "      right_window = self.best_dev_loss_epoch + self.nr_epoch_neigh + 1\n",
        "      \n",
        "      best_loss_neigh_epoch_stats = self.train_stats[left_window:right_window]\n",
        "      \n",
        "      best_epoch_stats = self.train_stats[self.best_dev_acc_epoch]\n",
        "      \n",
        "      if len(best_loss_neigh_epoch_stats) > 1:\n",
        "        best_loss_neigh_best_acc = max(best_loss_neigh_epoch_stats, key=attrgetter(\"dev_accuracy\"))\n",
        "\n",
        "        best_epoch_stats = self.train_stats[self.best_dev_loss_epoch]\n",
        "\n",
        "        if best_loss_neigh_best_acc.dev_accuracy > best_epoch_stats.dev_accuracy and best_loss_neigh_best_acc.train_accuracy >= best_loss_neigh_best_acc.dev_accuracy and best_loss_neigh_best_acc.train_loss <= best_loss_neigh_best_acc.dev_loss:\n",
        "          best_epoch_stats = best_loss_neigh_best_acc\n",
        "      \n",
        "      self.best_epoch_stats = best_epoch_stats\n",
        "\n",
        "      should_early_stop = True\n",
        "    \n",
        "    return should_early_stop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dkUk7fSo3nM",
        "colab_type": "text"
      },
      "source": [
        "#### Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7NjP7j_oe_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from operator import itemgetter, attrgetter\n",
        "\n",
        "def get_epoch_metrics(sess, model, batch_it, aspect_label_name):\n",
        "  sess.run(model.metrics_init_op)\n",
        "  \n",
        "  for batch_item in batch_it:\n",
        "    model.update_batch_metrics(sess, batch_item, 1., aspect_label_name)\n",
        "  \n",
        "  conf_matrix, accuracy, loss, summary = model.get_epoch_metrics(sess, aspect_label_name)\n",
        "  \n",
        "  return Epoch_Metrics(conf_matrix, accuracy, loss), summary\n",
        "  \n",
        "def trainer(model, model_config, trainer_config, train_stat_tracker, vocab, aspect_label_name, logdir_aspect_name, iteration, verbose=False):\n",
        "    \n",
        "  train_file_writer = tf.summary.FileWriter(logdir_aspect_name / \"train\", tf.get_default_graph())            \n",
        "  dev_file_writer = tf.summary.FileWriter(logdir_aspect_name / \"dev\", tf.get_default_graph())\n",
        "  saver = tf.train.Saver()\n",
        "  model_checkpoint_exists = tf.train.checkpoint_exists(trainer_config.saver_path)\n",
        "    \n",
        "  with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
        "\n",
        "      if model_checkpoint_exists:\n",
        "        saver.restore(sess, trainer_config.saver_path)\n",
        "      else:\n",
        "        model.init.run()\n",
        "\n",
        "        train_set_vocab = vocab.vectors.numpy()\n",
        "        sess.run(model.embedding_init, feed_dict={model.embedding_placeholder: train_set_vocab})\n",
        "                                                                                              \n",
        "      for epoch in range(trainer_config.n_epochs):\n",
        "          \n",
        "          for train_batch_item in train_batch_it:\n",
        "            model.perform_train_op(sess, train_batch_item, model_config.droput_keep_prob, model_config.droput_keep_prob_input, model_config.droput_keep_prob_cell, aspect_label_name)\n",
        "          \n",
        "          train_metrics, train_summary = get_epoch_metrics(sess, model, train_batch_it, aspect_label_name)\n",
        "          dev_metrics, dev_summary = get_epoch_metrics(sess, model, dev_batch_it, aspect_label_name)\n",
        "          test_metrics, test_summary = get_epoch_metrics(sess, model, test_batch_it, aspect_label_name)\n",
        "          \n",
        "          train_file_writer.add_summary(train_summary, epoch)\n",
        "          dev_file_writer.add_summary(dev_summary, epoch)\n",
        "\n",
        "#           train_file_writer.flush()\n",
        "#           dev_file_writer.flush()\n",
        "\n",
        "          if epoch != 0 and epoch % trainer_config.nr_epochs_to_save_model_checkpoint == 0:\n",
        "            saver.save(sess, trainer_config.saver_path)\n",
        "\n",
        "            print(\"Checkpoint saved!\")\n",
        "        \n",
        "          epoch_stat = train_stat_tracker.record_train_stats(sess, epoch, train_metrics, dev_metrics, test_metrics, iteration)\n",
        "          \n",
        "          if verbose:\n",
        "            print(epoch_stat)\n",
        "          \n",
        "          is_last_epoch = epoch + 1 == trainer_config.n_epochs\n",
        "          \n",
        "          if train_stat_tracker.check_early_stopping(is_last_epoch):\n",
        "            \n",
        "            print(\"best_epoch_stats\")\n",
        "            print(train_stat_tracker.best_epoch_stats)\n",
        "            break\n",
        "      \n",
        "      if train_stat_tracker.best_params:\n",
        "        print(\"Restoring model weights...\")\n",
        "        _restore_model_params(sess, train_stat_tracker.best_params)\n",
        "        train_metrics, train_summary = get_epoch_metrics(sess, model, train_batch_it, aspect_label_name)\n",
        "        dev_metrics, dev_summary = get_epoch_metrics(sess, model, dev_batch_it, aspect_label_name)\n",
        "        test_metrics, test_summary = get_epoch_metrics(sess, model, test_batch_it, aspect_label_name)\n",
        "#         print(\"Train accuracy:\", train_accuracy, \"Dev accuracy\", dev_accuracy, \"Train loss:\", train_loss, \"Dev loss:\", dev_loss, \"Test acc:\", test_accuracy)\n",
        "      \n",
        "      train_file_writer.close()\n",
        "      dev_file_writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooyeAiaYUHCX",
        "colab_type": "text"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0hafxUnqQRl",
        "colab_type": "text"
      },
      "source": [
        "#### Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzYnKDzLzjik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Single aspect learning\n",
        "class RNN_config(object):\n",
        "  n_neurons = 100\n",
        "  n_outputs = 5\n",
        "  learning_rate = 0.005\n",
        "  n_layers = 2\n",
        "  cell_type = \"gru\" \n",
        "  use_bidirectional = True\n",
        "  use_extra_dense_layer = True\n",
        "  weight_decay = 0.01\n",
        "  \n",
        "  droput_keep_prob=0.9\n",
        "  droput_keep_prob_input = 1.0\n",
        "  droput_keep_prob_cell = 0.5\n",
        "  \n",
        "# MTL\n",
        "class MTL_RNN_config(object):\n",
        "  n_neurons = 100\n",
        "  n_outputs = 5\n",
        "  learning_rate = 0.005\n",
        "  n_layers = 2\n",
        "  cell_type = \"gru\" \n",
        "  use_bidirectional = True\n",
        "  use_extra_dense_layer = True\n",
        "  weight_decay = 0\n",
        "\n",
        "  droput_keep_prob = 0.9\n",
        "  droput_keep_prob_input = 1.0\n",
        "  droput_keep_prob_cell = 0.5\n",
        "  \n",
        "  aspect_weights = [1.0] * 7\n",
        "  \n",
        "# Conditional\n",
        "class Cond_RNN_config(object):\n",
        "  n_neurons = 100\n",
        "  n_outputs = 5\n",
        "  learning_rate = 0.005\n",
        "  n_layers = 2\n",
        "  cell_type = \"gru\"\n",
        "  use_extra_dense_layer = True\n",
        "  weight_decay = 0\n",
        "\n",
        "  droput_keep_prob = 0.5\n",
        "  droput_keep_prob_input = 1.0\n",
        "  droput_keep_prob_cell = 0.5\n",
        "  \n",
        "# CNN\n",
        "class CNNConfig(object):\n",
        "  n_neurons = 100\n",
        "  n_outputs = 5\n",
        "  \n",
        "  num_filters = 128\n",
        "  kernel_size = 5\n",
        "  \n",
        "  learning_rate = 1e-3\n",
        "\n",
        "  weight_decay = 0\n",
        "\n",
        "  droput_keep_prob=0.5\n",
        "  droput_keep_prob_input = 0.9\n",
        "  droput_keep_prob_cell = 0.5\n",
        "  n_layers = 2\n",
        "  cell_type = \"gru\" \n",
        "  use_bidirectional = True\n",
        "\n",
        "class Trainer_config(object):\n",
        "  def __init__(self):\n",
        "    self.n_epochs = 50\n",
        "    self.nr_epochs_to_save_model_checkpoint = 300\n",
        "    self.saver_path = \"model_ckpt_y_rec_impact_clarity\"\n",
        "    self.logdir = Path(\"multiple_aspects\")\n",
        "    self.runs = 10\n",
        "    self.stats_runs_path = env_var.stats_path / \"tensorflow_models_stats.p\"\n",
        "#     self.stats_runs_path = Path(\"tensorflow_models_stats.p\")\n",
        "    \n",
        "  def set_logdir(self, path):\n",
        "    self.logdir = env_var.stats_path / \"tboard\" / path\n",
        "\n",
        "class Preproc_config(object):\n",
        "  seq_length = 350 #n_steps\n",
        "  vocab_max_size = 20000\n",
        "  vocab_min_freq = 5\n",
        "  pretrained_vectors = torchtext.vocab.Vectors(\"glove.6B.100d.txt\", env_var.embeddings_path)\n",
        "  comments_field_name = 'comments'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4TZZ1Hni_SQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model(model_name=None):\n",
        "  if model_name == \"MTL_RNN\":\n",
        "    return RNN_multi_task_model, MTL_RNN_config\n",
        "  \n",
        "  if model_name == \"Cond_RNN\":\n",
        "    return RNN_conditional_model, Cond_RNN_config\n",
        "  \n",
        "  if model_name == \"CNN\":\n",
        "    return CNN, CNNConfig\n",
        "  \n",
        "  return RNN_single_aspect_model, RNN_config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYBHY6hPU8q-",
        "colab_type": "text"
      },
      "source": [
        "#### Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0d7lZAL_bzNV",
        "colab": {}
      },
      "source": [
        "# import shutil\n",
        "# shutil.rmtree(\"gdrive/My Drive/thesis/tensorflow_stats/tboard\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCPsZCUU7ygk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %tensorboard --logdir {Trainer_config.logdir}\n",
        "%tensorboard --logdir 'gdrive/My Drive/thesis/tensorflow_stats_random/tboard/CNN/recommendation'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtLSyVNfqTkW",
        "colab_type": "text"
      },
      "source": [
        "#### Main run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBd-aeTmGPai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_intermediate_paths =[Path(\"\"),\n",
        "                             Path(\"strength_weak_sections\"),\n",
        "                             Path(\"manual_labeled_strength_weak_sections\"),\n",
        "                             Path(\"manual_labeled_strength_weak_sections/first_section\"),\n",
        "                             Path(\"manual_labeled_strength_weak_sections/strength_weak_sections\"),\n",
        "                             Path(\"manual_labeled_strength_weak_sections/strength_weak_sections_len_limit\"),\n",
        "                             Path(\"manual_labeled_strength_weak_sections/summary_strength_weak_sections\"),\n",
        "                             Path(\"manual_labeled_strength_weak_sections/summary_strength_weak_sections/clarity_sentences\"),\n",
        "                             Path(\"manual_labeled_strength_weak_sections/weak_section\"),\n",
        "                             Path(\"manual_labeled_strength_weak_sections/weak_strength_sections_len_limit\"),\n",
        "                             Path(\"manual_labeled_strength_weak_sections/aug_stren_weak_sections_len_limit\"),\n",
        "                             Path(\"manual_labeled_strength_weak_sections/strength_section\"),\n",
        "                             Path(\"acl_abstracts\")\n",
        "                            ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocCmnuVlHKYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import operator\n",
        "\n",
        "class Model_stats(object):\n",
        "  def __init__(self, dataset_intermediate_path, model_name, aspect_label_name):\n",
        "    self.dataset_intermediate_path = dataset_intermediate_path\n",
        "    self.model_name = model_name\n",
        "    self.aspect_label_name = aspect_label_name\n",
        "    self.stats = []\n",
        "    \n",
        "  def compute_mean_metrics(self):\n",
        "    top_3_dev_acc_stats = sorted(self.stats, key=operator.attrgetter('dev_accuracy'), reverse=True)[:3]\n",
        "    \n",
        "    self.mean_convergence_epoch = np.mean([stat.epoch for stat in self.stats]) \n",
        "    \n",
        "    self.mean_dev_accuracy = np.mean([stat.dev_accuracy for stat in self.stats]) \n",
        "    self.mean_test_accuracy = np.mean([stat.test_accuracy for stat in self.stats]) \n",
        "    \n",
        "    self.std_dev_accuracy = np.std([stat.dev_accuracy for stat in self.stats]) \n",
        "    self.std_test_accuracy = np.std([stat.test_accuracy for stat in self.stats]) \n",
        "    \n",
        "    self.top_3_mean_dev_accuracy = np.mean([stat.dev_accuracy for stat in top_3_dev_acc_stats]) \n",
        "    self.top_3_mean_test_accuracy = np.mean([stat.test_accuracy for stat in top_3_dev_acc_stats]) \n",
        "    \n",
        "    self.best_dev_acc_stats = top_3_dev_acc_stats[0]\n",
        "    self.best_epoch_dev_conf_matrix = self.best_dev_acc_stats.dev_conf_matrix\n",
        "    self.best_epoch_test_conf_matrix = self.best_dev_acc_stats.test_conf_matrix    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbcixQy0g5f4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "class Stats_Manager(object):\n",
        "  def __init__(self, stats_runs_path):\n",
        "    self.stats_runs_path = stats_runs_path\n",
        "  \n",
        "  def create_or_restore_runs_stats_obj(self, dataset_intermediate_paths, model_names, aspect_label_names):\n",
        "    self.models_stats = []\n",
        "\n",
        "    if self.stats_runs_path.is_file():\n",
        "      with open(self.stats_runs_path, \"rb\") as input_file:\n",
        "        self.models_stats = pickle.load(input_file)\n",
        "\n",
        "    self.should_restore_to_last_run = len(self.models_stats)\n",
        "\n",
        "    if self.should_restore_to_last_run:\n",
        "      last_run = self.models_stats[-1]\n",
        "\n",
        "      self.last_path_idx = dataset_intermediate_paths.index(last_run.dataset_intermediate_path)\n",
        "      self.last_model_idx = model_names.index(last_run.model_name)\n",
        "      self.last_aspect_idx = aspect_label_names.index(last_run.aspect_label_name)\n",
        "\n",
        "  def should_skip_current_run(self):\n",
        "    _should_skip_current_run = False\n",
        "    \n",
        "    if self.should_restore_to_last_run:\n",
        "      _should_skip_current_run = path_idx < self.last_path_idx or (path_idx <= self.last_path_idx and model_idx < self.last_model_idx) or (path_idx <= self.last_path_idx and model_idx <= self.last_model_idx and aspect_idx <= self.last_aspect_idx)\n",
        "    \n",
        "    return _should_skip_current_run\n",
        "  \n",
        "  def update(self, model_stats):\n",
        "    self.models_stats.append(model_stats)\n",
        "    with open(self.stats_runs_path, \"wb\") as output_file:\n",
        "      pickle.dump(self.models_stats, output_file)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4T3DcdU21ag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_names = [\"CNN\", \"RNN\", \"MTL_RNN\", \"Cond_RNN\"]\n",
        "aspect_label_names = ['RECOMMENDATION', 'IMPACT', 'SUBSTANCE', 'CLARITY', 'REVIEWER_CONFIDENCE', 'SOUNDNESS_CORRECTNESS', 'ORIGINALITY']\n",
        "\n",
        "preproc_config = Preproc_config()\n",
        "trainer_config = Trainer_config()\n",
        "\n",
        "stats_manager = Stats_Manager(trainer_config.stats_runs_path)\n",
        "stats_manager.create_or_restore_runs_stats_obj(dataset_intermediate_paths, model_names, aspect_label_names)\n",
        "  \n",
        "for path_idx, ds_inter_path in enumerate(dataset_intermediate_paths):\n",
        "  \n",
        "  env_var.set_dataset_intermediate_path(ds_inter_path)\n",
        "  trainer_config.set_logdir(ds_inter_path)\n",
        "  \n",
        "  print(\"DS_PATH:\", env_var.dataset_path)\n",
        "  \n",
        "  for model_idx, model_name in enumerate(model_names):\n",
        "    print(model_name)\n",
        "    for aspect_idx, aspect_label_name in enumerate(aspect_label_names):\n",
        "      print(aspect_label_name)\n",
        "      \n",
        "      if stats_manager.should_skip_current_run():\n",
        "        continue\n",
        "      \n",
        "      model_stats = Model_stats(ds_inter_path, model_name, aspect_label_name)\n",
        "      \n",
        "      for iteration in range(trainer_config.runs):\n",
        "        env_var.set_ds_fname(iteration)\n",
        "        \n",
        "        print(env_var.train_dataset_filename, env_var.dev_dataset_filename)\n",
        "        \n",
        "        vocab, vocab_size, embedding_dim, train_batch_it, dev_batch_it, test_batch_it = preprocessing_pipeline(preproc_config.seq_length, preproc_config.vocab_max_size, preproc_config.vocab_min_freq, preproc_config.pretrained_vectors, preproc_config.comments_field_name)\n",
        "                \n",
        "        tf.reset_default_graph()\n",
        "    \n",
        "        model, model_config = get_model(model_name)\n",
        "\n",
        "        model = model(model_config, vocab_size, embedding_dim, preproc_config.seq_length)\n",
        "\n",
        "        logdir_aspect_name = trainer_config.logdir/ model_name/ aspect_label_name.lower() / str(iteration)\n",
        "      \n",
        "        train_stat_tracker = Train_Stat_Tracker()\n",
        "\n",
        "        trainer(model, model_config, trainer_config, train_stat_tracker, vocab, aspect_label_name, logdir_aspect_name, iteration, verbose=False)\n",
        "        \n",
        "        model_stats.stats.append(train_stat_tracker.best_epoch_stats)\n",
        "        \n",
        "      model_stats.compute_mean_metrics()\n",
        "      stats_manager.update(model_stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5YfQ_dhqdee",
        "colab_type": "text"
      },
      "source": [
        "#### Model summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maxU2RVi3egH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "277e2194-304b-4270-ca66-eea30581727b"
      },
      "source": [
        "preproc_config = Preproc_config()\n",
        "trainer_config = Trainer_config()\n",
        "\n",
        "ds_inter_path = dataset_intermediate_paths[0]\n",
        "env_var.set_dataset_intermediate_path(ds_inter_path)\n",
        "trainer_config.set_logdir(ds_inter_path)\n",
        "\n",
        "print(\"DS_PATH:\", env_var.dataset_path)\n",
        "\n",
        "vocab, vocab_size, embedding_dim, train_batch_it, dev_batch_it, test_batch_it = preprocessing_pipeline(preproc_config.seq_length, preproc_config.vocab_max_size, preproc_config.vocab_min_freq, preproc_config.pretrained_vectors, preproc_config.comments_field_name)\n",
        "  \n",
        "model_names = [\"CNN\", \"RNN\", \"MTL_RNN\", \"Cond_RNN\"]\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DS_PATH: gdrive/My Drive/thesis/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DylgaQsiRXsP",
        "colab_type": "code",
        "outputId": "fbb097ea-c87c-45e7-91f8-160b1a87bf25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "import tensorflow.contrib.slim as slim\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "model_name = \"Cond_RNN\"\n",
        "\n",
        "model, model_config = get_model(model_name)\n",
        "model = model(model_config, vocab_size, embedding_dim, preproc_config.seq_length)\n",
        "\n",
        "def model_summary():\n",
        "  model_vars = tf.trainable_variables()\n",
        "  slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n",
        "#   slim.model_analyzer.analyze_ops(tf.get_default_graph(), print_info=True)\n",
        "# dir(slim)\n",
        "model_summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------\n",
            "Variables: name (type shape) [size]\n",
            "---------\n",
            "embedding:0 (float32_ref 1926x100) [192600, bytes: 770400]\n",
            "bidirectional_rnn/fw/multi_rnn_cell/cell_0/fw_aspectcell_gru_0/gates/kernel:0 (float32_ref 200x200) [40000, bytes: 160000]\n",
            "bidirectional_rnn/fw/multi_rnn_cell/cell_0/fw_aspectcell_gru_0/gates/bias:0 (float32_ref 200) [200, bytes: 800]\n",
            "bidirectional_rnn/fw/multi_rnn_cell/cell_0/fw_aspectcell_gru_0/candidate/kernel:0 (float32_ref 200x100) [20000, bytes: 80000]\n",
            "bidirectional_rnn/fw/multi_rnn_cell/cell_0/fw_aspectcell_gru_0/candidate/bias:0 (float32_ref 100) [100, bytes: 400]\n",
            "bidirectional_rnn/bw/multi_rnn_cell/cell_0/bw_aspectcell_gru_0/gates/kernel:0 (float32_ref 200x200) [40000, bytes: 160000]\n",
            "bidirectional_rnn/bw/multi_rnn_cell/cell_0/bw_aspectcell_gru_0/gates/bias:0 (float32_ref 200) [200, bytes: 800]\n",
            "bidirectional_rnn/bw/multi_rnn_cell/cell_0/bw_aspectcell_gru_0/candidate/kernel:0 (float32_ref 200x100) [20000, bytes: 80000]\n",
            "bidirectional_rnn/bw/multi_rnn_cell/cell_0/bw_aspectcell_gru_0/candidate/bias:0 (float32_ref 100) [100, bytes: 400]\n",
            "bidirectional_rnn/fw/multi_rnn_cell/cell_0/fwcell_gru_0/gates/kernel:0 (float32_ref 200x200) [40000, bytes: 160000]\n",
            "bidirectional_rnn/fw/multi_rnn_cell/cell_0/fwcell_gru_0/gates/bias:0 (float32_ref 200) [200, bytes: 800]\n",
            "bidirectional_rnn/fw/multi_rnn_cell/cell_0/fwcell_gru_0/candidate/kernel:0 (float32_ref 200x100) [20000, bytes: 80000]\n",
            "bidirectional_rnn/fw/multi_rnn_cell/cell_0/fwcell_gru_0/candidate/bias:0 (float32_ref 100) [100, bytes: 400]\n",
            "bidirectional_rnn/fw/multi_rnn_cell/cell_1/fwcell_gru_1/gates/kernel:0 (float32_ref 200x200) [40000, bytes: 160000]\n",
            "bidirectional_rnn/fw/multi_rnn_cell/cell_1/fwcell_gru_1/gates/bias:0 (float32_ref 200) [200, bytes: 800]\n",
            "bidirectional_rnn/fw/multi_rnn_cell/cell_1/fwcell_gru_1/candidate/kernel:0 (float32_ref 200x100) [20000, bytes: 80000]\n",
            "bidirectional_rnn/fw/multi_rnn_cell/cell_1/fwcell_gru_1/candidate/bias:0 (float32_ref 100) [100, bytes: 400]\n",
            "bidirectional_rnn/bw/multi_rnn_cell/cell_0/bwcell_gru_0/gates/kernel:0 (float32_ref 200x200) [40000, bytes: 160000]\n",
            "bidirectional_rnn/bw/multi_rnn_cell/cell_0/bwcell_gru_0/gates/bias:0 (float32_ref 200) [200, bytes: 800]\n",
            "bidirectional_rnn/bw/multi_rnn_cell/cell_0/bwcell_gru_0/candidate/kernel:0 (float32_ref 200x100) [20000, bytes: 80000]\n",
            "bidirectional_rnn/bw/multi_rnn_cell/cell_0/bwcell_gru_0/candidate/bias:0 (float32_ref 100) [100, bytes: 400]\n",
            "bidirectional_rnn/bw/multi_rnn_cell/cell_1/bwcell_gru_1/gates/kernel:0 (float32_ref 200x200) [40000, bytes: 160000]\n",
            "bidirectional_rnn/bw/multi_rnn_cell/cell_1/bwcell_gru_1/gates/bias:0 (float32_ref 200) [200, bytes: 800]\n",
            "bidirectional_rnn/bw/multi_rnn_cell/cell_1/bwcell_gru_1/candidate/kernel:0 (float32_ref 200x100) [20000, bytes: 80000]\n",
            "bidirectional_rnn/bw/multi_rnn_cell/cell_1/bwcell_gru_1/candidate/bias:0 (float32_ref 100) [100, bytes: 400]\n",
            "fully_connected/weights:0 (float32_ref 200x100) [20000, bytes: 80000]\n",
            "fully_connected/biases:0 (float32_ref 100) [100, bytes: 400]\n",
            "fully_connected_1/weights:0 (float32_ref 100x100) [10000, bytes: 40000]\n",
            "fully_connected_1/biases:0 (float32_ref 100) [100, bytes: 400]\n",
            "fully_connected_2/weights:0 (float32_ref 100x5) [500, bytes: 2000]\n",
            "fully_connected_2/biases:0 (float32_ref 5) [5, bytes: 20]\n",
            "Total size of variables: 585105\n",
            "Total bytes of variables: 2340420\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}