{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ulmfit_model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "cTsYGYwpQ57Q",
        "het58H6lv_WO",
        "r0yPC3Jg_YxM",
        "qcUv7rrbCEH1",
        "uk2unOnY7zvz"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mihayy/review_aspect_score_prediction/blob/master/models/ulmfit_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOSY0AhJ5gXp",
        "colab_type": "text"
      },
      "source": [
        "## ULMFit model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTB7ocM45lFK",
        "colab_type": "text"
      },
      "source": [
        "### Load fast.ai"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQBKqoKUq7ku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!curl https://course.fast.ai/setup/colab | bash"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYqBt3sfqmql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai import *\n",
        "from fastai.train import *\n",
        "\n",
        "from fastai.text import *\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cON7ED-IWWAa",
        "colab_type": "text"
      },
      "source": [
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoUBrQ0xL1un",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0_g6a7sEJbY",
        "colab_type": "text"
      },
      "source": [
        "### Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH7RLrf8U8qk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Config(object):\n",
        "  def __init__(self):\n",
        "    self.gdrive_working_dir = Path(\"gdrive/My Drive/thesis\")\n",
        "\n",
        "    self.embeddings_path = self.gdrive_working_dir/ \"embeddings\"\n",
        "\n",
        "#     self.dataset_intermediate_path = Path(\"manual_labeled_strength_weak_sections/summary_strength_weak_sections/clarity_sentences/mandatory_clarity_sentences\")\n",
        "    self.dataset_intermediate_path = Path(\"\")\n",
        "\n",
        "#     self.dataset_intermediate_path = Path(\"manual_labeled_strength_weak_sections\")\n",
        "#     self.dataset_intermediate_path = Path(\"acl_abstracts\")\n",
        "    self.dataset_path = self.gdrive_working_dir/ \"data\" / self.dataset_intermediate_path\n",
        "\n",
        "    self.train_dataset_filename = \"train_dataset.csv\"\n",
        "    self.dev_dataset_filename = \"dev_dataset.csv\"\n",
        "    self.test_dataset_filename = \"acl_dev_test.csv\"\n",
        "    \n",
        "    self.train_ds = \"train_ds_\"\n",
        "    self.test_ds = \"test_ds_\"\n",
        "\n",
        "    self.aspects = [\"RECOMMENDATION\", \"REVIEWER_CONFIDENCE\", \"SOUNDNESS_CORRECTNESS\", \"IMPACT\", \"SUBSTANCE\", \"CLARITY\", \"ORIGINALITY\"]\n",
        "#     self.aspects = [\"RECOMMENDATION\", \"REVIEWER_CONFIDENCE\"]\n",
        "    \n",
        "    self.stats_path = self.gdrive_working_dir / \"ulmfit_stats_random\"\n",
        "#     self.stats_path = Path(\"\")\n",
        "    \n",
        "  def set_dataset_intermediate_path(self, intermediate_path):\n",
        "    self.dataset_intermediate_path = intermediate_path\n",
        "    self.dataset_path = self.gdrive_working_dir/ \"data\" / self.dataset_intermediate_path\n",
        "\n",
        "  def set_ds_fname(self, iteration):\n",
        "    self.train_dataset_filename = self.train_ds + str(iteration) + \".csv\"\n",
        "    self.dev_dataset_filename = self.test_ds + str(iteration) + \".csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGb6APcqEjhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_var = Config()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSWe2oHK_YO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lm_encoder = \"lm_encoder\"\n",
        "lm_ds_pkl = \"lm_ds.pkl\"\n",
        "\n",
        "dataset_intermediate_paths =[Path(\"\"),\n",
        "                             Path(\"strength_weak_sections\"),\n",
        "                             Path(\"manual_labeled_strength_weak_sections\"),\n",
        "                             Path(\"manual_labeled_strength_weak_sections/first_section\"),\n",
        "                             Path(\"manual_labeled_strength_weak_sections/strength_weak_sections\"),\n",
        "                             Path(\"manual_labeled_strength_weak_sections/strength_weak_sections_len_limit\"),\n",
        "                             Path(\"manual_labeled_strength_weak_sections/summary_strength_weak_sections\"),\n",
        "                             Path(\"manual_labeled_strength_weak_sections/summary_strength_weak_sections/clarity_sentences\"),\n",
        "                             Path(\"manual_labeled_strength_weak_sections/weak_section\"),\n",
        "                             Path(\"manual_labeled_strength_weak_sections/weak_strength_sections_len_limit\"),\n",
        "                             Path(\"manual_labeled_strength_weak_sections/aug_stren_weak_sections_len_limit\"),\n",
        "                             Path(\"manual_labeled_strength_weak_sections/strength_section\"),\n",
        "                             Path(\"acl_abstracts\")\n",
        "                            ]\n",
        "\n",
        "# dataset_intermediate_paths =[Path(\"acl_abstracts\")]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTsYGYwpQ57Q",
        "colab_type": "text"
      },
      "source": [
        "## Dataset splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgxWwQdKdfsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# for path_idx, ds_inter_path in enumerate(dataset_intermediate_paths):\n",
        "#   print(ds_inter_path)\n",
        "#   env_var.set_dataset_intermediate_path(ds_inter_path)\n",
        "\n",
        "#   df = pd.read_csv(env_var.dataset_path / \"acl_reviews.csv\")\n",
        "  \n",
        "#   for run_idx in range(10):\n",
        "#     train_fname = \"train_ds_\" + str(run_idx) + \".csv\"\n",
        "#     test_fname = \"test_ds_\" + str(run_idx) + \".csv\"\n",
        "\n",
        "#     train_set, test_set = train_test_split(df, test_size=0.2, random_state=run_idx)\n",
        "#     train_set.to_csv(env_var.dataset_path / train_fname, index=False)\n",
        "#     test_set.to_csv(env_var.dataset_path / test_fname, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZ2D-psSf_Wk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for path_idx, ds_inter_path in enumerate(dataset_intermediate_paths):\n",
        "#   print(ds_inter_path)\n",
        "#   env_var.set_dataset_intermediate_path(ds_inter_path)\n",
        "  \n",
        "#   lm_df = get_lm_df(env_var)\n",
        "# #   lm_df.to_csv(env_var.dataset_path / \"acl_reviews.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "het58H6lv_WO"
      },
      "source": [
        "## Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0yPC3Jg_YxM",
        "colab_type": "text"
      },
      "source": [
        "### Create LM DS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ueoku5Tmv_WP",
        "colab": {}
      },
      "source": [
        "def get_lm_df(env_var):\n",
        "  train_ds_df = pd.read_csv(env_var.dataset_path / env_var.train_dataset_filename)\n",
        "  dev_ds_df = pd.read_csv(env_var.dataset_path / env_var.dev_dataset_filename)\n",
        "  test_ds_df = pd.read_csv(env_var.dataset_path / env_var.test_dataset_filename)\n",
        "\n",
        "  return pd.concat([train_ds_df, dev_ds_df, test_ds_df])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcKudx5NAMYu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for path_idx, ds_inter_path in enumerate(dataset_intermediate_paths):\n",
        "  print(ds_inter_path)\n",
        "  env_var.set_dataset_intermediate_path(ds_inter_path)\n",
        "  \n",
        "  lm_df = get_lm_df(env_var)\n",
        "\n",
        "  data_lm = (TextList.from_df(lm_df, env_var.dataset_path, cols='comments')\n",
        "                     .split_by_rand_pct()\n",
        "                     .label_for_lm()\n",
        "                     .databunch())\n",
        "  \n",
        "#   data_lm.save(\"lm_ds.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcUv7rrbCEH1",
        "colab_type": "text"
      },
      "source": [
        "### Train LM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O83_J4vO3QH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for ds_idx, ds_inter_path in enumerate(dataset_intermediate_paths):\n",
        "  env_var.set_dataset_intermediate_path(ds_inter_path)\n",
        "  print(env_var.dataset_path)\n",
        "  \n",
        "  data_lm = load_data(env_var.dataset_path, lm_ds_pkl)  \n",
        "  learn = language_model_learner(data_lm, arch=AWD_LSTM, drop_mult=0.5, path=env_var.dataset_path)\n",
        "  lm_acc_cb = callbacks.SaveModelCallback(learn, every=\"improvement\", monitor='accuracy', name='lm_acc_cb')\n",
        "  \n",
        "  learn.fit_one_cycle(1, 5e-2, moms=(0.8,0.7))\n",
        "  learn.unfreeze()\n",
        "  \n",
        "  if ds_idx == 11:\n",
        "    learn.fit_one_cycle(10, 2e-3, moms=(0.8,0.7), callbacks=[lm_acc_cb])\n",
        "    learn.fit_one_cycle(5, 2e-4, moms=(0.8,0.7), callbacks=[lm_acc_cb])\n",
        "    \n",
        "  elif ds_idx == 1:\n",
        "    learn.fit_one_cycle(7, 2e-3, moms=(0.8,0.7), callbacks=[lm_acc_cb])\n",
        "    learn.fit_one_cycle(5, 2e-4, moms=(0.8,0.7), callbacks=[lm_acc_cb])\n",
        "  \n",
        "  else:\n",
        "    learn.fit_one_cycle(5, 2e-3, moms=(0.8,0.7), callbacks=[lm_acc_cb])\n",
        "\n",
        "  learn.save_encoder(lm_encoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dolb2RdqXnzW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lm_results = []\n",
        "\n",
        "for ds_inter_path in dataset_intermediate_paths:\n",
        "  print(ds_inter_path)\n",
        "  env_var.set_dataset_intermediate_path(ds_inter_path)\n",
        "  data_lm = load_data(env_var.dataset_path, lm_ds_pkl)  \n",
        "  learn = language_model_learner(data_lm, arch=AWD_LSTM, drop_mult=0.5, path=env_var.dataset_path)\n",
        "  learn.load_encoder(lm_encoder)\n",
        "  lm_results.append(learn.validate(learn.data.valid_dl))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRumaYNjT3bA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lm_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HCIi4ZIKv_W6"
      },
      "source": [
        "## Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-0sY4dSdOvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_for_train_valid_split(env_var):\n",
        "  train_ds_df = pd.read_csv(env_var.dataset_path / env_var.train_dataset_filename)\n",
        "  dev_ds_df = pd.read_csv(env_var.dataset_path / env_var.dev_dataset_filename)\n",
        "  \n",
        "  train_ds_df[\"is_valid\"] = False\n",
        "  dev_ds_df[\"is_valid\"] = True\n",
        "\n",
        "  return pd.concat([train_ds_df, dev_ds_df])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCREHzdl8A0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Preproc_config(object):\n",
        "  def __init__(self):\n",
        "#     self.train_dev_bs = 49\n",
        "    self.train_dev_bs = 40\n",
        "    self.test_bs = 27\n",
        "    self.classes = [1,2,3,4,5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4m-tIImkb-rs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Trian_config(object):\n",
        "  def __init__(self):\n",
        "    self.runs = 10\n",
        "\n",
        "    self.best_loss_cb_name = \"best_loss_cb\"\n",
        "    self.best_loss_cb_name_pth = self.best_loss_cb_name + \".pth\"\n",
        "    \n",
        "    self.best_loss_cb_1_name = \"best_loss_cb_1\"\n",
        "    self.best_loss_cb_1_name_pth = self.best_loss_cb_1_name + \".pth\"\n",
        "    \n",
        "    self.best_loss_cb_2_name = \"best_loss_cb_2\"\n",
        "    self.best_loss_cb_2_name_pth = self.best_loss_cb_2_name + \".pth\"\n",
        "    \n",
        "    self.stats_runs_path = env_var.stats_path / \"ulmfit_stats.p\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZZSv2q8ppR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing_pipeline(aspect_idx, preproc_config):\n",
        "\n",
        "  data_lm = load_data(env_var.dataset_path, lm_ds_pkl)\n",
        "\n",
        "  train_dev_ds_df = prepare_for_train_valid_split(env_var)\n",
        "\n",
        "  data = (TextList.from_df(path=env_var.dataset_path, df=train_dev_ds_df, cols='comments', vocab=data_lm.vocab)\n",
        "                  .split_from_df(col=8)\n",
        "                  .label_from_df(cols=aspect_idx, classes=preproc_config.classes)\n",
        "                  .databunch(bs=preproc_config.train_dev_bs))\n",
        "  \n",
        "  \n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f6W11jPGhY98",
        "colab": {}
      },
      "source": [
        "def get_current_epoch(learn):\n",
        "  return np.argmin(learn.recorder.val_losses) + 1\n",
        "\n",
        "def trainer(data, model_stats, trian_config, iteration):\n",
        "  moms = (0.8,0.7)\n",
        "  \n",
        "  learn = text_classifier_learner(data, AWD_LSTM, drop_mult=0.5, path=env_var.dataset_path, silent=True)\n",
        "  save_best_loss_cb = callbacks.SaveModelCallback(learn, every=\"improvement\", monitor='valid_loss', name=trian_config.best_loss_cb_name)\n",
        "  save_best_loss_cb_1 = callbacks.SaveModelCallback(learn, every=\"improvement\", monitor='valid_loss', name=trian_config.best_loss_cb_1_name)\n",
        "  save_best_loss_cb_2 = callbacks.SaveModelCallback(learn, every=\"improvement\", monitor='valid_loss', name=trian_config.best_loss_cb_2_name)\n",
        "  \n",
        "  learn.load_encoder(lm_encoder)\n",
        "\n",
        "  learn.unfreeze()\n",
        "  \n",
        "  learn.fit_one_cycle(2, 1e-2, moms=moms, callbacks=[save_best_loss_cb])\n",
        "\n",
        "  start_epoch = 3\n",
        "  \n",
        "  learn.fit_one_cycle(10, 5e-3, moms=moms, callbacks=[save_best_loss_cb_1])\n",
        "  \n",
        "  stat = get_best_cycle_stats(learn, model_stats, start_epoch, iteration)\n",
        "  \n",
        "  learn.load(save_best_loss_cb.name)\n",
        "  \n",
        "  learn.fit_one_cycle(10, 8e-3, moms=moms, callbacks=[save_best_loss_cb_2])\n",
        "  \n",
        "  curr_best_cb = save_best_loss_cb_2\n",
        "  \n",
        "  if save_best_loss_cb_1.best < save_best_loss_cb_2.best:\n",
        "    print(\"First is better\")\n",
        "    learn.load(save_best_loss_cb_1.name)\n",
        "    curr_best_cb = save_best_loss_cb_1\n",
        "  else:\n",
        "    stat = get_best_cycle_stats(learn, model_stats, start_epoch, iteration)\n",
        "  \n",
        "  print(env_var.dataset_path, model_stats.aspect_label_name)\n",
        "  \n",
        "  print(stat)\n",
        "  \n",
        "  stats = [stat]\n",
        "  \n",
        "  start_epoch += get_current_epoch(learn)\n",
        "  \n",
        "  print(save_best_loss_cb_1.best, save_best_loss_cb_2.best)  \n",
        "  \n",
        "  learn.fit_one_cycle(3, 1e-3, moms=moms, callbacks=[save_best_loss_cb])\n",
        "           \n",
        "  if curr_best_cb.best < save_best_loss_cb.best:\n",
        "    print(curr_best_cb.best, save_best_loss_cb.best)\n",
        "    print(\"undo 1e-3 cycle\")\n",
        "    learn.load(curr_best_cb.name)\n",
        "  else:\n",
        "    stat_1 = get_best_cycle_stats(learn, model_stats, start_epoch, iteration)\n",
        "    stats.append(stat_1)\n",
        "    start_epoch += get_current_epoch(learn)\n",
        "    print(stat_1)\n",
        "\n",
        "  learn.fit_one_cycle(6, 1e-4, moms=moms, callbacks=[save_best_loss_cb])\n",
        "\n",
        "  stat_2 = get_best_cycle_stats(learn, model_stats, start_epoch, iteration)\n",
        "  stats.append(stat_2)\n",
        "\n",
        "  best_stat = max(stats, key=operator.attrgetter('dev_accuracy'))\n",
        "  print(\"BEST THING\")\n",
        "  print(iteration, best_stat)\n",
        "  \n",
        "  learn.destroy()\n",
        "  return best_stat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjYjhUausUKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Stat():\n",
        "  def __init__(self, epoch, dev_accuracy, dev_loss, dev_conf_matrix, iteration):\n",
        "    self.epoch = epoch    \n",
        "    self.dev_accuracy = dev_accuracy\n",
        "    self.dev_loss = dev_loss\n",
        "    self.dev_conf_matrix = dev_conf_matrix\n",
        "    self.iteration = iteration\n",
        "    \n",
        "  def __str__(self):\n",
        "    return f'{self.epoch} Test accuracy: {self.dev_accuracy} Test loss: {self.dev_loss}'\n",
        "\n",
        "nr_epoch_neigh = 3\n",
        "\n",
        "def get_conf_matrices(learn, test_dataset):\n",
        "  dev_conf_matrix = learn.interpret().confusion_matrix()\n",
        "  \n",
        "  initial_valid_dl = learn.data.valid_dl\n",
        "  \n",
        "  learn.data.valid_dl = test_dataset.train_dl\n",
        "  test_conf_matrix = learn.interpret().confusion_matrix()\n",
        "  learn.data.valid_dl = initial_valid_dl\n",
        "  \n",
        "  return dev_conf_matrix, test_conf_matrix\n",
        "\n",
        "def get_best_cycle_stats(learn, model_stats, start_epoch=0, iteration=0):\n",
        "  \n",
        "  dev_loss_stats = learn.recorder.val_losses\n",
        "  dev_acc_stats = learn.recorder.metrics\n",
        "  \n",
        "  dev_acc_stats = np.squeeze(dev_acc_stats)\n",
        "\n",
        "  best_loss_idx = np.argmin(dev_loss_stats)\n",
        "  best_loss = dev_loss_stats[best_loss_idx]\n",
        "  best_loss_acc = dev_acc_stats[best_loss_idx]\n",
        "  \n",
        "  model_stats.check_for_best_model(learn, best_loss, best_loss_acc, iteration)\n",
        "  \n",
        "  left_window = best_loss_idx - nr_epoch_neigh\n",
        "\n",
        "  if left_window < 0: \n",
        "    left_window = 0\n",
        "\n",
        "  right_window = best_loss_idx + 1 + nr_epoch_neigh\n",
        "\n",
        "  best_loss_neigh_best_acc_idx = np.argmax(dev_acc_stats[left_window:right_window])\n",
        "\n",
        "  best_epoch_idx = left_window + best_loss_neigh_best_acc_idx\n",
        "  \n",
        "  if best_loss_acc == dev_acc_stats[best_epoch_idx]:\n",
        "    best_epoch_idx = best_loss_idx\n",
        "  \n",
        "  dev_conf_matrix = []\n",
        "  \n",
        "  if best_epoch_idx == best_loss_idx:\n",
        "    dev_conf_matrix = learn.interpret().confusion_matrix()\n",
        "  \n",
        "  dev_loss = dev_loss_stats[best_epoch_idx]\n",
        "  dev_acc = dev_acc_stats[best_epoch_idx]\n",
        "  \n",
        "  return Stat(best_epoch_idx + start_epoch, dev_acc, dev_loss, dev_conf_matrix, iteration)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocCmnuVlHKYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import operator\n",
        "\n",
        "class Model_stats(object):\n",
        "  def __init__(self, dataset_intermediate_path, aspect_label_name):\n",
        "    self.dataset_intermediate_path = dataset_intermediate_path\n",
        "    self.aspect_label_name = aspect_label_name\n",
        "    self.stats = []\n",
        "    \n",
        "    self.best_model_path = \"random_best_\" + aspect_label_name\n",
        "    self.best_saved_model_dev_acc = 0\n",
        "    self.best_saved_model_dev_loss = 0\n",
        "    self.best_iteration = 0\n",
        "  \n",
        "  def check_for_best_model(self, learn, dev_loss, dev_accuracy, iteration):\n",
        "    \n",
        "    if self.best_saved_model_dev_acc < dev_accuracy or (self.best_saved_model_dev_acc <= dev_accuracy and self.best_saved_model_dev_loss < dev_loss):\n",
        "      learn.save(self.best_model_path)\n",
        "      self.best_saved_model_dev_acc = dev_accuracy\n",
        "      self.best_saved_model_dev_loss = dev_loss\n",
        "      self.best_iteration = iteration\n",
        "  \n",
        "  def compute_mean_metrics(self):\n",
        "    top_3_dev_acc_stats = sorted(self.stats, key=operator.attrgetter('dev_accuracy'), reverse=True)[:3]\n",
        "    \n",
        "    self.mean_convergence_epoch = np.mean([stat.epoch for stat in self.stats]) \n",
        "    \n",
        "    self.mean_dev_accuracy = np.mean([stat.dev_accuracy for stat in self.stats]) \n",
        "    \n",
        "    self.std_dev_accuracy = np.std([stat.dev_accuracy for stat in self.stats]) \n",
        "    \n",
        "    self.top_3_mean_dev_accuracy = np.mean([stat.dev_accuracy for stat in top_3_dev_acc_stats]) \n",
        "    \n",
        "    self.best_dev_acc_stats = top_3_dev_acc_stats[0]\n",
        "    self.best_epoch_dev_conf_matrix = self.best_dev_acc_stats.dev_conf_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbcixQy0g5f4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "class Stats_Manager(object):\n",
        "  def __init__(self, stats_runs_path):\n",
        "    self.stats_runs_path = stats_runs_path\n",
        "  \n",
        "  def create_or_restore_runs_stats_obj(self, dataset_intermediate_paths, aspect_label_names):\n",
        "    self.models_stats = []\n",
        "\n",
        "    if self.stats_runs_path.is_file():\n",
        "      with open(self.stats_runs_path, \"rb\") as input_file:\n",
        "        self.models_stats = pickle.load(input_file)\n",
        "\n",
        "    self.should_restore_to_last_run = len(self.models_stats)\n",
        "\n",
        "    if self.should_restore_to_last_run:\n",
        "      last_run = self.models_stats[-1]\n",
        "\n",
        "      self.last_path_idx = dataset_intermediate_paths.index(last_run.dataset_intermediate_path)\n",
        "      self.last_aspect_idx = aspect_label_names.index(last_run.aspect_label_name)\n",
        "\n",
        "  def should_skip_current_run(self):\n",
        "    _should_skip_current_run = False\n",
        "    \n",
        "    if self.should_restore_to_last_run:\n",
        "      _should_skip_current_run = path_idx < self.last_path_idx or (path_idx <= self.last_path_idx and aspect_idx <= self.last_aspect_idx)\n",
        "    \n",
        "    return _should_skip_current_run\n",
        "  \n",
        "  def update(self, model_stats):\n",
        "    self.models_stats.append(model_stats)\n",
        "    with open(self.stats_runs_path, \"wb\") as output_file:\n",
        "      pickle.dump(self.models_stats, output_file)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbnmKbGr60jA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "def regulate_cuda_memory():\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TojF45RkL4lv",
        "colab_type": "text"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCOF0L6fIFez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "def remove_tmp_cb(path):\n",
        "  if path.is_file():\n",
        "    os.remove(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uu6KTTM9oplK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trian_config = Trian_config()\n",
        "\n",
        "stats_manager = Stats_Manager(trian_config.stats_runs_path)\n",
        "stats_manager.create_or_restore_runs_stats_obj(dataset_intermediate_paths, env_var.aspects)\n",
        "\n",
        "preproc_config = Preproc_config()\n",
        "  \n",
        "for path_idx, ds_inter_path in enumerate(dataset_intermediate_paths):\n",
        "  \n",
        "  env_var.set_dataset_intermediate_path(ds_inter_path)\n",
        "  \n",
        "  print(\"DS_PATH:\", env_var.dataset_path)\n",
        "    \n",
        "  for aspect_idx, aspect_label_name in enumerate(env_var.aspects):\n",
        "    print(env_var.dataset_path, aspect_label_name)\n",
        "\n",
        "    if stats_manager.should_skip_current_run():\n",
        "      continue\n",
        "      \n",
        "    model_stats = Model_stats(ds_inter_path, aspect_label_name)\n",
        "\n",
        "    for iteration in range(trian_config.runs):\n",
        "      env_var.set_ds_fname(iteration)\n",
        "      \n",
        "      print(env_var.train_dataset_filename, env_var.dev_dataset_filename)\n",
        "      \n",
        "      data = preprocessing_pipeline(aspect_idx + 1, preproc_config)\n",
        "      \n",
        "      best_epoch_stats = trainer(data, model_stats, trian_config, iteration)\n",
        "      \n",
        "      model_stats.stats.append(best_epoch_stats)\n",
        "\n",
        "    model_stats.compute_mean_metrics()\n",
        "    stats_manager.update(model_stats)\n",
        "    regulate_cuda_memory()\n",
        "    \n",
        "  remove_tmp_cb(env_var.dataset_path / \"models\" / trian_config.best_loss_cb_name_pth)  \n",
        "  remove_tmp_cb(env_var.dataset_path / \"models\" / trian_config.best_loss_cb_1_name_pth)  \n",
        "  remove_tmp_cb(env_var.dataset_path / \"models\" / trian_config.best_loss_cb_2_name_pth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdZ_ctramtLI",
        "colab_type": "text"
      },
      "source": [
        "## Simple stats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYxBKeQxzC7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_intermediate_paths_grouped = [Path('.'),\n",
        " Path('manual_labeled_strength_weak_sections'),\n",
        " Path('strength_weak_sections'),\n",
        " Path('manual_labeled_strength_weak_sections/summary_strength_weak_sections'),\n",
        " Path('manual_labeled_strength_weak_sections/strength_weak_sections'),\n",
        " Path('manual_labeled_strength_weak_sections/first_section'),\n",
        " Path('manual_labeled_strength_weak_sections/weak_section'),\n",
        " Path('manual_labeled_strength_weak_sections/strength_section'),\n",
        " Path('manual_labeled_strength_weak_sections/strength_weak_sections_len_limit'),\n",
        " Path('manual_labeled_strength_weak_sections/weak_strength_sections_len_limit'),\n",
        " Path('manual_labeled_strength_weak_sections/aug_strength_weak_sections_len_limit'),\n",
        " Path('manual_labeled_strength_weak_sections/summary_strength_weak_sections/clarity_sentences'),\n",
        " Path('acl_abstracts')]\n",
        "\n",
        "comments_manipulations = [\"comments\", \n",
        "  \"manual\", \n",
        "  \"str_weak\",           \n",
        "  \"manual_sum_str_weak\", \n",
        "  \"manual_str_weak\",  \n",
        "  \"manual_sum\", \n",
        "  \"manual_weak\", \n",
        "  \"manual_str\", \n",
        "  \"manual_str_weak_limit\", \n",
        "  \"manual_weak_str_limit\", \n",
        "  \"manual_limit_aug\", \n",
        "  \"manual_clarity\", \n",
        "  \"abstract\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n098KZtnaqX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trian_config = Trian_config()\n",
        "\n",
        "preproc_config = Preproc_config()\n",
        "\n",
        "venue_len_mean = []\n",
        "venue_len_std = []\n",
        "\n",
        "env_var.set_ds_fname(0)\n",
        "\n",
        "for path_idx, ds_inter_path in enumerate(dataset_intermediate_paths_grouped):\n",
        "  \n",
        "  env_var.set_dataset_intermediate_path(ds_inter_path)\n",
        "  \n",
        "  acl_reviews = pd.read_csv(env_var.dataset_path / env_var.train_dataset_filename)\n",
        "  text_column = acl_reviews['comments']\n",
        "      \n",
        "  venue_reviews_len = []\n",
        "    \n",
        "  for review_text in text_column:\n",
        "      venue_reviews_len.append(len(review_text.split()))\n",
        "\n",
        "  mean = int(np.mean(venue_reviews_len))\n",
        "  std = int(np.std(venue_reviews_len))\n",
        "\n",
        "  venue_len_mean.append(mean)\n",
        "  venue_len_std.append(std)\n",
        "\n",
        "  min_len = np.min(venue_reviews_len)\n",
        "  max_len = np.max(venue_reviews_len)\n",
        "  \n",
        "  print(env_var.dataset_intermediate_path, \": mean\", mean, \"std\", std, \"min_len\", min_len, \"max_len\", max_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qVQYloRpr-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(12,5))\n",
        "x_pos = np.arange(len(comments_manipulations))\n",
        "\n",
        "ax.bar(x_pos, venue_len_mean, yerr=venue_len_std, align='center', alpha=0.5, ecolor='black', capsize=7)\n",
        "ax.set_ylabel('Content length')\n",
        "ax.set_xticks(x_pos)\n",
        "ax.set_xticklabels(comments_manipulations)\n",
        "ax.set_title(\"ACL content's length\")\n",
        "ax.yaxis.grid(True)\n",
        "\n",
        "fig.autofmt_xdate()\n",
        "plt.xlabel(\"Type of content\")\n",
        "plt.tight_layout()\n",
        "plt.savefig('aclcontentlen.jpg', dpi=300)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSxzqUN5YhQ0",
        "colab_type": "text"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGRZWEQi1I48",
        "colab_type": "code",
        "outputId": "b84901ef-a281-433d-d7ba-939365be8c50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import matplotlib.cm as cm\n",
        "\n",
        "txt_ci = TextClassificationInterpretation.from_learner(learn)\n",
        "test_text = \"We achieve the state of the art with our method.\"\n",
        "txt_ci.show_intrinsic_attention(test_text,cmap=cm.Purples)\n",
        "learn.predict(test_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"font-family: monospace;\"><span title=\"0.405\" style=\"background-color: rgba(181, 180, 215, 0.5);\">xxbos</span> <span title=\"0.220\" style=\"background-color: rgba(223, 222, 237, 0.5);\">xxmaj</span> <span title=\"0.753\" style=\"background-color: rgba(105, 80, 162, 0.5);\">we</span> <span title=\"1.000\" style=\"background-color: rgba(63, 0, 125, 0.5);\">achieve</span> <span title=\"0.148\" style=\"background-color: rgba(235, 233, 243, 0.5);\">the</span> <span title=\"0.302\" style=\"background-color: rgba(205, 205, 228, 0.5);\">state</span> <span title=\"0.132\" style=\"background-color: rgba(238, 236, 244, 0.5);\">of</span> <span title=\"0.097\" style=\"background-color: rgba(242, 240, 246, 0.5);\">the</span> <span title=\"0.223\" style=\"background-color: rgba(222, 222, 237, 0.5);\">art</span> <span title=\"0.179\" style=\"background-color: rgba(230, 229, 240, 0.5);\">with</span> <span title=\"0.318\" style=\"background-color: rgba(201, 202, 226, 0.5);\">our</span> <span title=\"0.191\" style=\"background-color: rgba(228, 227, 239, 0.5);\">method</span> <span title=\"0.077\" style=\"background-color: rgba(244, 242, 248, 0.5);\">.</span></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Category 3, tensor(2), tensor([0.1142, 0.0325, 0.6043, 0.2002, 0.0488]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    }
  ]
}